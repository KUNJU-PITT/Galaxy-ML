<tool id="keras_model_builder" name="Add an optimizer, loss and fit parameters" version="@VERSION@">
  <description>to the deep learning architecture and compile</description>
  <macros>
		<import>main_macros.xml</import>
    <import>keras_macros.xml</import>
  </macros>
	<expand macro="python_requirements"/>
  <expand macro="macro_stdio"/>
  <version_command>echo "@VERSION@"</version_command>
  <command>
    <![CDATA[
    python '$__tool_directory__/keras_deep_learning.py'
           '$inputs'
           'keras_model_builder'
           '$outfile'
           '$mode_selection.infile_json'
           #if $mode_selection.mode_type == 'prefitted'
           '$mode_selection.infile_weights'
           #end if

    ]]>
  </command>
  <configfiles>
    <inputs name="inputs"/>
  </configfiles>
  <inputs>
    <conditional name="mode_selection">
      <param name="mode_type" type="select" label="Choose a building mode">
        <option value="train_model" selected="true">Build a training model</option>
        <option value="prefitted">Load a pretrained model for prediction</option>
      </param>
      <when value="train_model">
        <param name="infile_json" type="data" format="json" label="Select the dataset containing model configurations (JSON)"/>
        <param name="learning_type" type="select" label="Do classification or regression?">
          <option value="keras_classifier">KerasGClassifier</option>
          <option value="keras_regressor">KerasGRegressor</option>
        </param>
        <section name="compile_params" title="Compile Parameters" expanded="true">
          <param name="loss" type="select" label="Select a loss function">
            <option value="mean_squared_error">mse / MSE/ mean_squared_error</option>
            <option value="mean_absolute_error">mae / MAE / mean_absolute_error</option>
            <option value="mean_absolute_percentage_error">mape / MAPE / mean_absolute_percentage_error</option>
            <option value="mean_squared_logarithmic_error">msle / MSLE / mean_squared_logarithmic_error</option>
            <option value="squared_hinge">squared_hinge</option>
            <option value="hinge">hinge</option>
            <option value="categorical_hinge">categorical_hinge</option>
            <option value="logcosh">logcosh</option>
            <option value="categorical_crossentropy" selected="true">categorical_crossentropy</option>
            <option value="sparse_categorical_crossentropy">sparse_categorical_crossentropy</option>
            <option value="binary_crossentropy">binary_crossentropy</option>
            <option value="kullback_leibler_divergence">kld / KLD / kullback_leibler_divergence</option>
            <option value="poisson">poisson</option>
            <option value="cosine_proximity">cosine / cosine_proximity</option>
          </param>
          <conditional name="optimizer_selection">
            <param name="optimizer_type" type="select" label="Select an optimizer">
              <option value="SGD" selected="true">SGD - Stochastic gradient descent optimizer </option>
              <option value="RMSprop">RMSprop - RMSProp optimizer </option>
              <option value="Adagrad">Adagrad - Adagrad optimizer </option>
              <option value="Adadelta">Adadelta - Adadelta optimizer </option>
              <option value="Adam">Adam - Adam optimizer </option>
              <option value="Adamax">Adamax - A variant of Adam based on the infinity norm </option>
              <option value="Nadam">Nadam - Nesterov Adam optimizer </option>
            </param>
            <when value="SGD">
              <expand macro="keras_optimizer_common">
                <param argument="momentum" type="float" value="0" optional="true" label="Momentum"
                    help="float >= 0. Parameter that accelerates SGD in the relevant direction and dampens oscillations."/>
                <param argument="decay" type="float" value="0" label="Decay" optional="true" help="float &gt;= 0. Learning rate decay over each update."/>
                <param argument="nesterov" type="boolean" truevalue="booltrue" falsevalue="boolfalse" optional="true" checked="false" label="Whether to apply Nesterov momentum"/>
              </expand>
            </when>
            <when value="RMSprop">
              <expand macro="keras_optimizer_common_more" lr="0.001">
                <param argument="rho" type="float" value="0.9" optional="true" label="rho" help="float &gt;= 0."/>
              </expand>
            </when>
            <when value="Adagrad">
              <expand macro="keras_optimizer_common_more" lr="0.001"/>
            </when>
            <when value="Adadelta">
              <expand macro="keras_optimizer_common_more" lr="1.0">
                <param argument="rho" type="float" value="0.95" optional="true" label="rho" help="float &gt;= 0."/>
              </expand>
            </when>
            <when value="Adam">
              <expand macro="keras_optimizer_common_more" lr="0.001">
                <param argument="beta_1" type="float" value="0.9" optional="true" label="beta_1" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="beta_2" type="float" value="0.999" optional="true" label="beta_2" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="amsgrad" type="boolean" truevalue="booltrue" falsevalue="boolfalse" optional="true" checked="false" label="Whether to apply the AMSGrad variant?" 
                    help="Refer to paper `On the Convergence of Adam and Beyond`"/>
              </expand>
            </when>
            <when value="Adamax">
              <expand macro="keras_optimizer_common_more" lr="0.002">
                <param argument="beta_1" type="float" value="0.9" optional="true" label="beta_1" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="beta_2" type="float" value="0.999" optional="true" label="beta_2" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
              </expand>
            </when>
            <when value="Nadam">
              <expand macro="keras_optimizer_common" lr="0.002">
                <param argument="beta_1" type="float" value="0.9" optional="true" label="beta_1" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="beta_2" type="float" value="0.999" optional="true" label="beta_2" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="epsilon" type="float" value="" label="epsilon" optional="true" help="Fuzz factor. If `None`, defaults to `K.epsilon()`"/>
                <param argument="schedule_decay" type="float" value="0.004" optional="true" label="schedule_decay" help="float, 0 &lt; beta &lt; 1."/>
              </expand>
            </when>
          </conditional>
          <!--param name="metrics" type="select" label="Select metrics"-->
        </section>
        <section name="fit_params" title="Fit Parameters" expanded="true">
          <param name="epochs" type="integer" value="1" min="1" label="epochs"/>
          <param name="batch_size" type="integer" value="" optional="true" label="batch_size" help="Integer or blank for None"/>
        </section>
        <param name="random_seed" type="integer" value="0" label="Random Seed" help="Integer or blank for None"/>
      </when>
      <when value="prefitted">
        <param name="infile_json" type="data" format="json" label="Select the dataset containing model configurations (JSON)"/>
        <param name="infile_weights" type="data" format="h5" label="Select the dataset containing keras layers weights"/>
      </when>
    </conditional>
	</inputs>
  <outputs>
    <data format="zip" name="outfile" label="Keras Model Builder  on ${on_string}"/>
  </outputs>
  <tests>
    <test>
      <conditional name="mode_selection">
        <param name="infile_json" value="keras01.json" ftype="json"/>
        <param name="learning_type" value="keras_regressor"/>
        <section name="fit_params">
          <param name="epochs" value="100"/>
        </section>
      </conditional>
      <output name="outfile" file="keras_model01" compare="sim_size" delta="5"/>
    </test>
		<test>
      <conditional name="mode_selection">
        <param name="infile_json" value="keras02.json" ftype="json"/>
        <section name="compile_params">
          <conditional name="optimizer_selection">
            <param name="optimizer_type" value="Adam"/>
          </conditional>
        </section>
        <section name="fit_params">
          <param name="epochs" value="100"/>
        </section>
      </conditional>
      <output name="outfile" file="keras_model02" compare="sim_size" delta="5"/>
    </test>
    <test>
      <conditional name="mode_selection">
        <param name="mode_type" value="prefitted"/>
        <param name="infile_json" value="keras03.json" ftype="json"/>
        <param name="infile_weights" value="keras_save_weights01.h5" ftype="h5"/>
      </conditional>
      <output name="outfile" file="keras_prefitted01.zip" compare="sim_size" delta="5"/>
    </test>
  </tests>
  <help>
      <![CDATA[
**Help**
**What it does**
Searches optimized parameter settings for an estimator or pipeline through either exhaustive grid cross validation search or Randomized cross validation search.
please refer to `Scikit-learn model_selection GridSearchCV`_, `Scikit-learn model_selection RandomizedSearchCV`_ and `Tuning hyper-parameters`_.

**Return**

Outputs `cv_results_` from SearchCV in a tabular dataset if no train_test_split, otherwise the test score(s). Besides, Output of the SearchCV object is optional.

**How to choose search patameters grid?**

Please refer to `svm`_, `linear_model`_, `ensemble`_, `naive_bayes`_, `tree`_, `neighbors`_ and `xgboost`_ for estimator parameters.
Refer to `sklearn.preprocessing`_, `feature_selection`_, `decomposition`_, `kernel_approximation`_, `cluster.FeatureAgglomeration`_
and `skrebate`_ for parameter in the pre-processing steps.

**Search parameter list** can be list, numpy array, or distribution. The evaluation of settings supports operations in Math, 
list comprehension, numpy.arange(np_arange), most numpy.random(e.g., np_random_uniform) and some scipy.stats(e.g., scipy_stats_zipf) classes or functions, and others.

Examples:

- [3, 5, 7, 9]

- list(range(50, 1001, 50))

- np_arange(0.01, 1, 0.1)

- np_random_choice(list(range(1, 51)) + [None], size=20)

- scipy_stats_randin(1, 11)

**Estimator / Preprocessor search (additional `:` in the front)**::

     : [sklearn_tree.DecisionTreeRegressor(), sklearn_tree.ExtraTreeRegressor()]

     : [sklearn_feature_selection.SelectKBest(), sklearn_feature_selection.VarianceThreshold(),
        skrebate_ReliefF(), sklearn_preprocessing.RobustScaler()]

**Hot number/keyword for preprocessors**::

    0   sklearn_preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)
    1   sklearn_preprocessing.Binarizer(copy=True, threshold=0.0)
    2   sklearn_preprocessing.Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
    3   sklearn_preprocessing.MaxAbsScaler(copy=True)
    4   sklearn_preprocessing.Normalizer(copy=True, norm='l2')
    5   sklearn_preprocessing.MinMaxScaler(copy=True, feature_range=(0, 1))
    6   sklearn_preprocessing.PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)
    7   sklearn_preprocessing.RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True, with_scaling=True)
    8   sklearn_feature_selection.SelectKBest(k=10, score_func=<function f_classif at 0x113806d90>)
    9   sklearn_feature_selection.GenericUnivariateSelect(mode='percentile', param=1e-05, score_func=<function f_classif at 0x113806d90>)
    10  sklearn_feature_selection.SelectPercentile(percentile=10, score_func=<function f_classif at 0x113806d90>)
    11  sklearn_feature_selection.SelectFpr(alpha=0.05, score_func=<function f_classif at 0x113806d90>)
    12  sklearn_feature_selection.SelectFdr(alpha=0.05, score_func=<function f_classif at 0x113806d90>)
    13  sklearn_feature_selection.SelectFwe(alpha=0.05, score_func=<function f_classif at 0x113806d90>)
    14  sklearn_feature_selection.VarianceThreshold(threshold=0.0)
    15  sklearn_decomposition.FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,
        noise_variance_init=None, random_state=0, svd_method='randomized', tol=0.01)
    16  sklearn_decomposition.FastICA(algorithm='parallel', fun='logcosh', fun_args=None,
        max_iter=200, n_components=None, random_state=0, tol=0.0001, w_init=None, whiten=True)
    17  sklearn_decomposition.IncrementalPCA(batch_size=None, copy=True, n_components=None, whiten=False)
    18  sklearn_decomposition.KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',
        fit_inverse_transform=False, gamma=None, kernel='linear', kernel_params=None, max_iter=None,
        n_components=None, random_state=0, remove_zero_eig=False, tol=0)
    19  sklearn_decomposition.LatentDirichletAllocation(batch_size=128, doc_topic_prior=None, evaluate_every=-1, learning_decay=0.7,
        learning_method=None, learning_offset=10.0, max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001, n_components=10,
        n_topics=None, perp_tol=0.1, random_state=0, topic_word_prior=None, total_samples=1000000.0, verbose=0)
    20  sklearn_decomposition.MiniBatchDictionaryLearning(alpha=1, batch_size=3, dict_init=None, fit_algorithm='lars',
        n_components=None, n_iter=1000, random_state=0, shuffle=True, split_sign=False, transform_algorithm='omp',
        transform_alpha=None, transform_n_nonzero_coefs=None, verbose=False)
    21  sklearn_decomposition.MiniBatchSparsePCA(alpha=1, batch_size=3, callback=None, method='lars', n_components=None,
        n_iter=100, random_state=0, ridge_alpha=0.01, shuffle=True, verbose=False)
    22  sklearn_decomposition.NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,
        n_components=None, random_state=0, shuffle=False, solver='cd', tol=0.0001, verbose=0)
    23  sklearn_decomposition.PCA(copy=True, iterated_power='auto', n_components=None, random_state=0, svd_solver='auto', tol=0.0, whiten=False)
    24  sklearn_decomposition.SparsePCA(U_init=None, V_init=None, alpha=1, max_iter=1000, method='lars',
        n_components=None, random_state=0, ridge_alpha=0.01, tol=1e-08, verbose=False)
    25  sklearn_decomposition.TruncatedSVD(algorithm='randomized', n_components=2, n_iter=5, random_state=0, tol=0.0)
    26  sklearn_kernel_approximation.Nystroem(coef0=None, degree=None, gamma=None, kernel='rbf',
        kernel_params=None, n_components=100, random_state=0)
    27  sklearn_kernel_approximation.RBFSampler(gamma=1.0, n_components=100, random_state=0)
    28  sklearn_kernel_approximation.AdditiveChi2Sampler(sample_interval=None, sample_steps=2)
    29  sklearn_kernel_approximation.SkewedChi2Sampler(n_components=100, random_state=0, skewedness=1.0)
    30  sklearn_cluster.FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto', connectivity=None,
        linkage='ward', memory=None, n_clusters=2, pooling_func=<function mean at 0x113078ae8>)
    31  skrebate_ReliefF(discrete_threshold=10, n_features_to_select=10, n_neighbors=100, verbose=False)
    32  skrebate_SURF(discrete_threshold=10, n_features_to_select=10, verbose=False)
    33  skrebate_SURFstar(discrete_threshold=10, n_features_to_select=10, verbose=False)
    34  skrebate_MultiSURF(discrete_threshold=10, n_features_to_select=10, verbose=False)
    35  skrebate_MultiSURFstar(discrete_threshold=10, n_features_to_select=10, verbose=False)
    'sk_prep_all':   All sklearn preprocessing estimators, i.e., 0-7
    'fs_all':        All feature_selection estimators, i.e., 8-14
    'decomp_all':    All decomposition estimators, i.e., 15-25
    'k_appr_all':    All kernel_approximation estimators, i.e., 26-29
    'reb_all':       All skrebate estimators, i.e., 31-35
    'all_0':         All except the imbalanced-learn samplers, i.e., 0-35
    'imb_all':       All imbalanced-learn sampling methods, i.e., 36-54.
                     **CAUTION**: Mix of imblearn and other preprocessors may not work.
     None:           opt out of preprocessor

Support mix (CAUTION: Mix of imblearn and other preprocessors may not work), e.g.::

     : [None, 'sk_prep_all', 22, 'k_appr_all', sklearn_feature_selection.SelectKBest(k=50)]



**Whether to do train_test_split?**

Please refer to `https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation`_


.. image:: https://scikit-learn.org/stable/_images/grid_search_cross_validation.png
    :height: 300
    :width: 400


.. _`Scikit-learn model_selection GridSearchCV`: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
.. _`Scikit-learn model_selection RandomizedSearchCV`: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html
.. _`Tuning hyper-parameters`: http://scikit-learn.org/stable/modules/grid_search.html

.. _`svm`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm
.. _`linear_model`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model
.. _`ensemble`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble
.. _`naive_bayes`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes
.. _`tree`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree
.. _`neighbors`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors
.. _`xgboost`: https://xgboost.readthedocs.io/en/latest/python/python_api.html

.. _`sklearn.preprocessing`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing
.. _`feature_selection`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection
.. _`decomposition`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition
.. _`kernel_approximation`: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.kernel_approximation
.. _`cluster.FeatureAgglomeration`: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html
.. _`skrebate`: https://epistasislab.github.io/scikit-rebate/using/
.. _`https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation`: https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation

      ]]>
  </help>
  <citations>
  </citations>
</tool>
