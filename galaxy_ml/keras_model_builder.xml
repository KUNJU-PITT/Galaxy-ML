<tool id="keras_model_builder" name="Create deep learning model" version="@VERSION@">
  <description>with an optimizer, loss function and fit parameters</description>
  <macros>
    <import>main_macros.xml</import>
    <import>keras_macros.xml</import>
  </macros>
  <expand macro="python_requirements"/>
  <expand macro="macro_stdio"/>
  <version_command>echo "@VERSION@"</version_command>
  <command>
    <![CDATA[
    python '$__tool_directory__/keras_deep_learning.py'
           '$inputs'
           'keras_model_builder'
           '$outfile'
           '$mode_selection.infile_json'
           #if $mode_selection.mode_type == 'prefitted'
           '$mode_selection.infile_weights'
           #end if

    ]]>
  </command>
  <configfiles>
    <inputs name="inputs"/>
  </configfiles>
  <inputs>
    <conditional name="mode_selection">
      <param name="mode_type" type="select" label="Choose a building mode">
        <option value="train_model" selected="true">Build a training model</option>
        <option value="prefitted">Load a pretrained model for prediction</option>
      </param>
      <when value="train_model">
        <param name="infile_json" type="data" format="json" label="Select the dataset containing model configurations (JSON)"/>
        <param name="learning_type" type="select" label="Do classification or regression?">
          <option value="KerasGClassifier">KerasGClassifier</option>
          <option value="KerasGRegressor">KerasGRegressor</option>
        </param>
        <section name="compile_params" title="Compile Parameters" expanded="true">
          <param name="loss" type="select" label="Select a loss function">
            <option value="binary_crossentropy" selected="true">binary_crossentropy</option>
            <option value="mean_squared_error">mse / MSE/ mean_squared_error</option>
            <option value="mean_absolute_error">mae / MAE / mean_absolute_error</option>
            <option value="mean_absolute_percentage_error">mape / MAPE / mean_absolute_percentage_error</option>
            <option value="mean_squared_logarithmic_error">msle / MSLE / mean_squared_logarithmic_error</option>
            <option value="squared_hinge">squared_hinge</option>
            <option value="hinge">hinge</option>
            <option value="categorical_hinge">categorical_hinge</option>
            <option value="logcosh">logcosh</option>
            <option value="categorical_crossentropy">categorical_crossentropy</option>
            <option value="sparse_categorical_crossentropy">sparse_categorical_crossentropy</option>
            <option value="kullback_leibler_divergence">kld / KLD / kullback_leibler_divergence</option>
            <option value="poisson">poisson</option>
            <option value="cosine_proximity">cosine / cosine_proximity</option>
          </param>
          <conditional name="optimizer_selection">
            <param name="optimizer_type" type="select" label="Select an optimizer">
              <option value="SGD" selected="true">SGD - Stochastic gradient descent optimizer </option>
              <option value="RMSprop">RMSprop - RMSProp optimizer </option>
              <option value="Adagrad">Adagrad - Adagrad optimizer </option>
              <option value="Adadelta">Adadelta - Adadelta optimizer </option>
              <option value="Adam">Adam - Adam optimizer </option>
              <option value="Adamax">Adamax - A variant of Adam based on the infinity norm </option>
              <option value="Nadam">Nadam - Nesterov Adam optimizer </option>
            </param>
            <when value="SGD">
              <expand macro="keras_optimizer_common">
                <param argument="momentum" type="float" value="0" optional="true" label="Momentum"
                    help="float >= 0. Parameter that accelerates SGD in the relevant direction and dampens oscillations."/>
                <param argument="decay" type="float" value="0" label="Decay" optional="true" help="float &gt;= 0. Learning rate decay over each update."/>
                <param argument="nesterov" type="boolean" truevalue="booltrue" falsevalue="boolfalse" optional="true" checked="false" label="Whether to apply Nesterov momentum"/>
              </expand>
            </when>
            <when value="RMSprop">
              <expand macro="keras_optimizer_common_more" lr="0.001">
                <param argument="rho" type="float" value="0.9" optional="true" label="rho" help="float &gt;= 0."/>
              </expand>
            </when>
            <when value="Adagrad">
              <expand macro="keras_optimizer_common_more" lr="0.001"/>
            </when>
            <when value="Adadelta">
              <expand macro="keras_optimizer_common_more" lr="1.0">
                <param argument="rho" type="float" value="0.95" optional="true" label="rho" help="float &gt;= 0."/>
              </expand>
            </when>
            <when value="Adam">
              <expand macro="keras_optimizer_common_more" lr="0.001">
                <param argument="beta_1" type="float" value="0.9" optional="true" label="beta_1" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="beta_2" type="float" value="0.999" optional="true" label="beta_2" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="amsgrad" type="boolean" truevalue="booltrue" falsevalue="boolfalse" optional="true" checked="false" label="Whether to apply the AMSGrad variant?" 
                    help="Refer to paper `On the Convergence of Adam and Beyond`"/>
              </expand>
            </when>
            <when value="Adamax">
              <expand macro="keras_optimizer_common_more" lr="0.002">
                <param argument="beta_1" type="float" value="0.9" optional="true" label="beta_1" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="beta_2" type="float" value="0.999" optional="true" label="beta_2" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
              </expand>
            </when>
            <when value="Nadam">
              <expand macro="keras_optimizer_common" lr="0.002">
                <param argument="beta_1" type="float" value="0.9" optional="true" label="beta_1" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="beta_2" type="float" value="0.999" optional="true" label="beta_2" help="float, 0 &lt; beta &lt; 1. Generally close to 1."/>
                <param argument="epsilon" type="float" value="" label="epsilon" optional="true" help="Fuzz factor. If `None`, defaults to `K.epsilon()`"/>
                <param argument="schedule_decay" type="float" value="0.004" optional="true" label="schedule_decay" help="float, 0 &lt; beta &lt; 1."/>
              </expand>
            </when>
          </conditional>
          <!--param name="metrics" type="select" label="Select metrics"-->
        </section>
        <section name="fit_params" title="Fit Parameters" expanded="true">
          <param name="epochs" type="integer" value="1" min="1" label="epochs"/>
          <param name="batch_size" type="integer" value="" optional="true" label="batch_size" help="Integer or blank for None"/>
          <param name="validation_freq" type="integer" value="1" optional="true" label="validation_freq" help="Integer only at current moment. If an integer, specifies how many training epochs to run before a new validation run is performed."/>
          <expand macro="keras_callbacks"/>
        </section>
        <param name="random_seed" type="integer" value="0" label="Random Seed" help="Integer or blank for None"/>
      </when>
      <when value="prefitted">
        <param name="infile_json" type="data" format="json" label="Select the dataset containing model configurations (JSON)"/>
        <param name="infile_weights" type="data" format="h5" label="Select the dataset containing keras layers weights"/>
      </when>
    </conditional>
  </inputs>
  <outputs>
    <data format="zip" name="outfile" label="Keras Model Builder  on ${on_string}"/>
  </outputs>
  <tests>
    <test>
      <conditional name="mode_selection">
        <param name="infile_json" value="keras01.json" ftype="json"/>
        <param name="learning_type" value="KerasGRegressor"/>
        <section name="fit_params">
          <param name="epochs" value="100"/>
        </section>
      </conditional>
      <output name="outfile" file="keras_model01" compare="sim_size" delta="5"/>
    </test>
    <test>
      <conditional name="mode_selection">
        <param name="infile_json" value="keras02.json" ftype="json"/>
        <section name="compile_params">
          <conditional name="optimizer_selection">
            <param name="optimizer_type" value="Adam"/>
          </conditional>
        </section>
        <section name="fit_params">
          <param name="epochs" value="100"/>
        </section>
      </conditional>
      <output name="outfile" file="keras_model02" compare="sim_size" delta="5"/>
    </test>
    <test>
      <conditional name="mode_selection">
        <param name="mode_type" value="prefitted"/>
        <param name="infile_json" value="keras03.json" ftype="json"/>
        <param name="infile_weights" value="keras_save_weights01.h5" ftype="h5"/>
      </conditional>
      <output name="outfile" file="keras_prefitted01.zip" compare="sim_size" delta="5"/>
    </test>
  </tests>
  <help>
      <![CDATA[
**Help**

**What it does**

Creates an estimator object (classifier or regressor) by using the architecture JSON from 'Create architecture' tool and adding an optimizer, loss function and other fit parameters. The fit parameters include the number of training epochs and batch size. Multiple attributes of an optimizer can also be set. A pre-trained deep learning model can also be used with this tool.

**Return**

An estimator object which can be used to train on a dataset.

**How to compile the architecture using this tool?**

1. Choose the architecture building mode. For example - choose "Build a training model".
2. Attach an architecture JSON file (obtained after executing "Create architecture" tool) which contains information about multiple layers.
3. Select a loss function. For example - for classification tasks, choose 'cross entropy' losses and for regression tasks, choose 'mean squared' or 'mean absolute' losses.
4. Choose an optimizer which minimizes the loss computed by the loss function. Multiple attributes of the chosen optimizer can be modified. 'RMSProp' and 'Adam' are some of the popular optimizers.
5. Insert the number of iterations (epochs) and the size of training batches (batch_size).
6. Execute the tool to get a compiled estimator object.

      ]]>
  </help>
  <citations>
  </citations>
</tool>
