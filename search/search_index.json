{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Galaxy-ML Galaxy-ML is a web machine learning end-to-end pipeline building framework, with special support to biomedical data. Under the management of unified scikit-learn APIs, cutting-edge machine learning libraries are combined together to provide thousands of different pipelines suitable for various needs. In the form of Galalxy tools, Galaxy-ML provides scalabe, reproducible and transparent machine learning computations. Key features simple web UI no coding or minimum coding requirement fast model deployment and model selection, specialized in hyperparameter tuning using GridSearchCV high level of parallel and automated computation Supported modules A typic machine learning pipeline is composed of a main estimator/model and optional preprocessing component(s). Model scikit-learn sklearn.ensemble sklearn.linear_model sklearn.naive_bayes sklearn.neighbors sklearn.svm sklearn.tree xgboost XGBClassifier XGBRegressor mlxtend StackingCVClassifier StackingClassifier StackingCVRegressor StackingRegressor Keras (Deep learning models are re-implemented to fully support sklearn APIs. Supports parameter, including layer subparameter, swaps or searches. Supports callbacks ) KerasGClassifier KerasGRegressor KerasGBatchClassifier (works best with online data generators, processing images, genomic sequences and so on) BinarizeTargetClassifier/BinarizeTargetRegressor IRAPSClassifier Preprocessor scikit-learn sklearn.preprocessing sklearn.feature_selection sklearn.decomposition sklearn.kernel_approximation sklearn.cluster imblanced-learn imblearn.under_sampling imblearn.over_sampling imblearn.combine skrebate ReliefF SURF SURFstar MultiSURF MultiSURFstar TDMScaler DyRFE/DyRFECV Z_RandomOverSampler GenomeOneHotEncoder ProteinOneHotEncoder FastaDNABatchGenerator FastaRNABatchGenerator FastaProteinBatchGenerator GenomicIntervalBatchGenerator GenomicVariantBatchGenerator ImageDataFrameBatchGenerator Installation APIs for models, preprocessors and utils implemented in Galaxy-ML can be installed separately. Installing using anaconda (recommended) conda install -c bioconda -c conda-forge Galaxy-ML Installing using pip pip install -U Galaxy-ML Installing from source python setup.py install Using source code inplace python setup.py build_ext --inplace To install Galaxy-ML tools in Galaxy, please refer to https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/. Examples for using Galaxy-ML APIs # handle imports from keras.models import Sequential from keras.layers import Dense, Activation from sklearn.model_selection import GridSearchCV from galaxy_ml.keras_galaxy_models import KerasGClassifier # build a DNN classifier model = Sequential() model.add(Dense(64)) model.add(Activation(\u2018relu')) model.add((Dense(1, activation=\u2018sigmoid\u2019))) config = model.get_config() classifier = KerasGClassifier(config, random_state=42) # clone a classifier clf = clone(classifier) # Get parameters params = clf.get_params() # Set parameters new_params = dict( epochs=60, lr=0.01, layers_1_Dense__config__kernel_initializer__config__seed=999, layers_0_Dense__config__kernel_initializer__config__seed=999 ) clf.set_params(**new_params) # model evaluation using GridSearchCV grid = GridSearchCV(clf, param_grid={}, scoring=\u2018roc_auc\u2019, cv=5, n_jobs=2) grid.fit(X, y)","title":"Home"},{"location":"#galaxy-ml","text":"Galaxy-ML is a web machine learning end-to-end pipeline building framework, with special support to biomedical data. Under the management of unified scikit-learn APIs, cutting-edge machine learning libraries are combined together to provide thousands of different pipelines suitable for various needs. In the form of Galalxy tools, Galaxy-ML provides scalabe, reproducible and transparent machine learning computations.","title":"Galaxy-ML"},{"location":"#key-features","text":"simple web UI no coding or minimum coding requirement fast model deployment and model selection, specialized in hyperparameter tuning using GridSearchCV high level of parallel and automated computation","title":"Key features"},{"location":"#supported-modules","text":"A typic machine learning pipeline is composed of a main estimator/model and optional preprocessing component(s).","title":"Supported modules"},{"location":"#model","text":"scikit-learn sklearn.ensemble sklearn.linear_model sklearn.naive_bayes sklearn.neighbors sklearn.svm sklearn.tree xgboost XGBClassifier XGBRegressor mlxtend StackingCVClassifier StackingClassifier StackingCVRegressor StackingRegressor Keras (Deep learning models are re-implemented to fully support sklearn APIs. Supports parameter, including layer subparameter, swaps or searches. Supports callbacks ) KerasGClassifier KerasGRegressor KerasGBatchClassifier (works best with online data generators, processing images, genomic sequences and so on) BinarizeTargetClassifier/BinarizeTargetRegressor IRAPSClassifier","title":"Model"},{"location":"#preprocessor","text":"scikit-learn sklearn.preprocessing sklearn.feature_selection sklearn.decomposition sklearn.kernel_approximation sklearn.cluster imblanced-learn imblearn.under_sampling imblearn.over_sampling imblearn.combine skrebate ReliefF SURF SURFstar MultiSURF MultiSURFstar TDMScaler DyRFE/DyRFECV Z_RandomOverSampler GenomeOneHotEncoder ProteinOneHotEncoder FastaDNABatchGenerator FastaRNABatchGenerator FastaProteinBatchGenerator GenomicIntervalBatchGenerator GenomicVariantBatchGenerator ImageDataFrameBatchGenerator","title":"Preprocessor"},{"location":"#installation","text":"APIs for models, preprocessors and utils implemented in Galaxy-ML can be installed separately.","title":"Installation"},{"location":"#installing-using-anaconda-recommended","text":"conda install -c bioconda -c conda-forge Galaxy-ML","title":"Installing using anaconda (recommended)"},{"location":"#installing-using-pip","text":"pip install -U Galaxy-ML","title":"Installing using pip"},{"location":"#installing-from-source","text":"python setup.py install","title":"Installing from source"},{"location":"#using-source-code-inplace","text":"python setup.py build_ext --inplace To install Galaxy-ML tools in Galaxy, please refer to https://galaxyproject.org/admin/tools/add-tool-from-toolshed-tutorial/.","title":"Using source code inplace"},{"location":"#examples-for-using-galaxy-ml-apis","text":"# handle imports from keras.models import Sequential from keras.layers import Dense, Activation from sklearn.model_selection import GridSearchCV from galaxy_ml.keras_galaxy_models import KerasGClassifier # build a DNN classifier model = Sequential() model.add(Dense(64)) model.add(Activation(\u2018relu')) model.add((Dense(1, activation=\u2018sigmoid\u2019))) config = model.get_config() classifier = KerasGClassifier(config, random_state=42) # clone a classifier clf = clone(classifier) # Get parameters params = clf.get_params() # Set parameters new_params = dict( epochs=60, lr=0.01, layers_1_Dense__config__kernel_initializer__config__seed=999, layers_0_Dense__config__kernel_initializer__config__seed=999 ) clf.set_params(**new_params) # model evaluation using GridSearchCV grid = GridSearchCV(clf, param_grid={}, scoring=\u2018roc_auc\u2019, cv=5, n_jobs=2) grid.fit(X, y)","title":"Examples for using Galaxy-ML APIs"},{"location":"CHANGELOG/","text":"Version 0.8.1 / tool_main: 1.0.8.1 / keras: 0.5.0 (12-12-2019) Changes Changes all predict_score to decision_functions . Bug Fixes Fixes _SafePickler desn't recognize binarize_target module. Fixes GridSearchCV has no predict_score issue. Version 0.8.0 / tool_main: 1.0.8.0 / keras: 0.5.0 (12-10-2019) New Features Adds circleci config for both api and tool tests. Adds train_test_split tool which supports shufflesplit, stratifiedshufflesplit, groupshufflesplit and orderedtarget split. Adds fitted_model_eval tool. Refactors binarize target estimators. There are a lot of improvements. One of them is that the estimator family now support most sklearn scorers. Adds clean_params in utils Adds cv_results_ outputs for nested inner CV and unfitted searchCV object from searchCV tool. Adds keras training and evaluation tool. Adds support of decision_function for binarize target classifiers. Adds matplotlib svg format option in ml_visualization_ex tool. Adds 'sklearn.ensemble.HistGradientBoostingClassifier' and 'sklearn.ensemble.HistGradientBoostingRegressor' Adds new regression scorer max_error . Upgade scikit-lean to v0.21.3, mlxtend to v0.17.0, imbalanced-learn to v0.5.0, keras to v2.3.1 and tensorflow to v1.15.0. Changes Replaces all generators' fit with set_processing_attrs . Raise ValueError instead of [0, 1] normalization when predictions from BinarizeTargetRegressor go out of range. Refactor iraps_classifier module. Binarize target estimators do the same prediction as the wrapped estimator. A delicated predict_score is made to work with binarize scorers. Changes precision-recall curve and ROC curve to take headers and upgrade plotly to v4.3.0 in ml_visualization_ex tool Change to dynamic output of pipeline or final main estimator Bug Fixes Fixes random_state error in _predict_generator . Fixes stale path issue by replace relative paths with full paths. Version 0.7.13 / tool_main: 1.0.7.12 / stacking: 0.2.0 / keras: 0.4.2 (09-18-2019) New Features Adds searchcv tools to output weights for deep learning models. Makes KerasGBatchClassifier.evalue to support multi-class and multi-label classification problem. Adds parameter verbose in KerasG models to output device placement. Adds metrics in keras model building tools. Makes train_test_eval tool. Makes GenomicVariantBatchGenerator . Makes model_prediction tool to support vcf file type. Adds plotly plotting tool facility for feature_importances , learning_curve , pr_curve and roc_curve . Adds _predict_generator to output y_true together with prediction results. Adds support of return_train_score for KerasGBatchClassifier in gridsearchcv. Adds ml_visualization.xml tool support many plots. Changes Changes dependency tensorflow to tensorflow-gpu . Moves all tools to folder tools . Makes sklearn.preprocessing.Imputer deprecated. Updates dependencies in requrements.txt . Refactor keras_model_config tool by grouping layer key words arguments. Refactor the preprocessors.py into folder structure. Bug Fixes Fixes KerasGBatchClassifier doesn't work with callbacks. Fixes GenomicIntervalBatchGenerator doesn't work in nested model validation. Fixes GenomicIntervalBatchGenerator failed for sequences in blacklist matches. Version 0.7.5 / tool_main: 1.0.7.5 / stacking: 0.2.0 / keras: 0.3.0 (07-09-2019) New Features Adds MIT license. Adds setup.py and requirement.txt for APIs installation. Makes Galaxy-ML APIs as a library and installable vis pypi and bioconda. Adds GenomicIntervalBatchGenerator , an online data generator that provides online genomic sequences transformation from a reference genome and intervals. By trying to offer the same functionalities of selene , GenomicIntervalBatchGenerator is implemented by, 1) reusing selene cython backend; 2) extending keras.utils.Sequence , multiple processing and queueing capable; 3) compatibilizing with sciKit-learn APIs, like KFold, GridSeearchCV, etc . GenomicIntervalBatchGenerator is supposed to be fast and memory-efficient. Adds parameter steps_per_epoch , validation_steps to BaseKerasModel . Adds parameter prediction_steps to KerasGBatchClassifier . Adds class_weight -like parameter class_positive_factor to KerasGBatchGenerator for imbalanced training. Changes Refactor fast array generators, introduced fit method. Refactor iraps_classifier random index generator, reduce fit time by about 45% Bug Fixes Version 0.7.1 / tool_main: 1.0.7.0 / stacking: 0.2.0 / keras: 0.3.0 (06-11-2019) New Features Adds validation_data into keras galaxy models and supports gridsearch and model_validation . Adds fasta sequence batch generator and makes FastaDNABatchGenerator , FastaRNABatchGenerator and FastaProteinBatchGenerator . Adds keras galaxy batch classifier and generator.flow . Adds keras_batch_models tool. Adds GenomeOneHotEncoder and ProteinOneHotEncoder to pipeline . Adds API documentation at https://goeckslab.github.io/Galaxy-ML/ . Extends BinarizeTarget Classifier/Regressor to support fit_params . Modifies read_columns function to avoid repeated input file reading. Changes Changes model_validation tool name. Bug Fixes Fixes CV groups file issue in searchcv tool. Fixes cheetah error in model_validation tool. Version 0.6.5 / tool_main: 1.0.6.2 / stacking: 0.2.0 / keras: 0.2.0 (05-26-2019) New Features Adds BinarizeTargetTransformer . Adds support of binarize_scorers to BaseSearchCV . Adds sklearn.ensemble.VotingClassifier and VotingRegressor (will be available sklearn v0.21). Enhances security of try_get_attr by adding check_def argument. Adds __all__ attribute together with try_get_attr to manage custom module and names. Adds keras callbacks. Now supports EarlyStopping , RemoteMonitor , TerminateOnNaN , ReduceLROnPlateau and partially support ModelCheckpoint , CSVLogger . Changes Pumps stacking_ensembles too to version 0.2.0. Changes KerasBatchClassifier to KerasGBatchClassifier . Bug Fixes Fix voting estimators duplicate naming problem. Version 0.6.0 / tool_main: 1.0.6.0 / keras: 0.2.0 (05-13-2019) New Features Adds Nested CV to searchcv tool. Adds BinarizeTargetClassifier .classifier_, BinarizeTargetRegressor .regressor_ and IRAPSClassifier .get_signature() in estimator_attributes tool. Reformat the output of corss_validate . Adds KerasBatchClassifier . Makes KerasGClassifier and KerasGRegressor support multi-dimension array. Changes Changes min value of n_splits from 2 to 1. Main Tool version changes on the last second number instead of the last one. Bug Fixes Fixes train_test_split which didn't work with default scoring. Version 0.5.0 / tool_main: 1.0.0.5 / keras: 0.2.0 (05-13-2019) New Features Extend binarize target scorers to support stacking estimators, i.e., use binarize target estimator as meta estimator. Adds cv_results attributes to estimator_attributes tool. Adds loading prefitted model for prediction in keras_model_builder tool. Adds save_weights and load_weights for keras classifier/regressor models. Merges keras model builder Changes Refactors the multiple scoring input for searchcv and simplify cv_results output. Refactors import system, get rid of exec import. Bug Fixes Fixes stacking estimators whitelist issue and other import issues. Fixes bases typo error in stacking ensembles tool Fixes multiple scoring error in train_test_split mode Version 0.4.0 / tool_main: 1.0.0.4 (04-29-2019) New Features Adds StackingCVClassifier , StackingClassifier and StackingRegressor to Stacking_ensembles tool, and makes explicit base estimator and meta estimator building options. Adds .gitattributes and .gitignore . Changes Changes extended_ensemble_ml.xml to stacking_ensembles.xml . Moves src to subfolder Galaxy-ML Bug Fixes Fix safepickler classobj issue Version 0.3.0/ tool_main: 1.0.0.3 (04-23-2019) New Features Makes RepeatedOrderedKFold . Makes train_test_split tool and adds train_test_split to searchcv tool. Adds jpickle to persist sklearn objects. Makes TDMScaler . Makes search parameter options in search_model_validation tool using from_dataset , the get_params output of estimator_attributes tool. Restructures estimator_attributes tool to be workflow friendly. Changes Separate OrderedKFold into model_validations module. Refactors SafePickler class and pickle white list loading system for better CPU and Memory efficiency. Separates feature_selector module out from utils . Bug Fixes Fix safepickler classobj issue Version 0.2.0 (03-24-2019) New Features SearchCV tool selects param from get_params() dataset. Adds extended_ensemble_ml tool which wraps StackingCVRegressor to ensemble machine learning. Extends estimator_attributes tool to output get_params() . Adds support of multipleprocessing in IRAPSCore . Changes Removes the limit of n_jobs=1 for IRAPSClassifier Changes named estimators in pipeline_builder tool. Use make_pipeline instead of Pipeline initiation. Bug Fixes Version 0.1.0 (03-15-2019)","title":"Change Log"},{"location":"CHANGELOG/#version-081-tool_main-1081-keras-050-12-12-2019","text":"","title":"Version 0.8.1 / tool_main: 1.0.8.1 / keras: 0.5.0 (12-12-2019)"},{"location":"CHANGELOG/#changes","text":"Changes all predict_score to decision_functions .","title":"Changes"},{"location":"CHANGELOG/#bug-fixes","text":"Fixes _SafePickler desn't recognize binarize_target module. Fixes GridSearchCV has no predict_score issue.","title":"Bug Fixes"},{"location":"CHANGELOG/#version-080-tool_main-1080-keras-050-12-10-2019","text":"","title":"Version 0.8.0 / tool_main: 1.0.8.0 / keras: 0.5.0 (12-10-2019)"},{"location":"CHANGELOG/#new-features","text":"Adds circleci config for both api and tool tests. Adds train_test_split tool which supports shufflesplit, stratifiedshufflesplit, groupshufflesplit and orderedtarget split. Adds fitted_model_eval tool. Refactors binarize target estimators. There are a lot of improvements. One of them is that the estimator family now support most sklearn scorers. Adds clean_params in utils Adds cv_results_ outputs for nested inner CV and unfitted searchCV object from searchCV tool. Adds keras training and evaluation tool. Adds support of decision_function for binarize target classifiers. Adds matplotlib svg format option in ml_visualization_ex tool. Adds 'sklearn.ensemble.HistGradientBoostingClassifier' and 'sklearn.ensemble.HistGradientBoostingRegressor' Adds new regression scorer max_error . Upgade scikit-lean to v0.21.3, mlxtend to v0.17.0, imbalanced-learn to v0.5.0, keras to v2.3.1 and tensorflow to v1.15.0.","title":"New Features"},{"location":"CHANGELOG/#changes_1","text":"Replaces all generators' fit with set_processing_attrs . Raise ValueError instead of [0, 1] normalization when predictions from BinarizeTargetRegressor go out of range. Refactor iraps_classifier module. Binarize target estimators do the same prediction as the wrapped estimator. A delicated predict_score is made to work with binarize scorers. Changes precision-recall curve and ROC curve to take headers and upgrade plotly to v4.3.0 in ml_visualization_ex tool Change to dynamic output of pipeline or final main estimator","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_1","text":"Fixes random_state error in _predict_generator . Fixes stale path issue by replace relative paths with full paths.","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0713-tool_main-10712-stacking-020-keras-042-09-18-2019","text":"","title":"Version 0.7.13 / tool_main: 1.0.7.12 / stacking: 0.2.0 / keras: 0.4.2 (09-18-2019)"},{"location":"CHANGELOG/#new-features_1","text":"Adds searchcv tools to output weights for deep learning models. Makes KerasGBatchClassifier.evalue to support multi-class and multi-label classification problem. Adds parameter verbose in KerasG models to output device placement. Adds metrics in keras model building tools. Makes train_test_eval tool. Makes GenomicVariantBatchGenerator . Makes model_prediction tool to support vcf file type. Adds plotly plotting tool facility for feature_importances , learning_curve , pr_curve and roc_curve . Adds _predict_generator to output y_true together with prediction results. Adds support of return_train_score for KerasGBatchClassifier in gridsearchcv. Adds ml_visualization.xml tool support many plots.","title":"New Features"},{"location":"CHANGELOG/#changes_2","text":"Changes dependency tensorflow to tensorflow-gpu . Moves all tools to folder tools . Makes sklearn.preprocessing.Imputer deprecated. Updates dependencies in requrements.txt . Refactor keras_model_config tool by grouping layer key words arguments. Refactor the preprocessors.py into folder structure.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_2","text":"Fixes KerasGBatchClassifier doesn't work with callbacks. Fixes GenomicIntervalBatchGenerator doesn't work in nested model validation. Fixes GenomicIntervalBatchGenerator failed for sequences in blacklist matches.","title":"Bug Fixes"},{"location":"CHANGELOG/#version-075-tool_main-1075-stacking-020-keras-030-07-09-2019","text":"","title":"Version 0.7.5 / tool_main: 1.0.7.5 / stacking: 0.2.0 / keras: 0.3.0 (07-09-2019)"},{"location":"CHANGELOG/#new-features_2","text":"Adds MIT license. Adds setup.py and requirement.txt for APIs installation. Makes Galaxy-ML APIs as a library and installable vis pypi and bioconda. Adds GenomicIntervalBatchGenerator , an online data generator that provides online genomic sequences transformation from a reference genome and intervals. By trying to offer the same functionalities of selene , GenomicIntervalBatchGenerator is implemented by, 1) reusing selene cython backend; 2) extending keras.utils.Sequence , multiple processing and queueing capable; 3) compatibilizing with sciKit-learn APIs, like KFold, GridSeearchCV, etc . GenomicIntervalBatchGenerator is supposed to be fast and memory-efficient. Adds parameter steps_per_epoch , validation_steps to BaseKerasModel . Adds parameter prediction_steps to KerasGBatchClassifier . Adds class_weight -like parameter class_positive_factor to KerasGBatchGenerator for imbalanced training.","title":"New Features"},{"location":"CHANGELOG/#changes_3","text":"Refactor fast array generators, introduced fit method. Refactor iraps_classifier random index generator, reduce fit time by about 45%","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_3","text":"","title":"Bug Fixes"},{"location":"CHANGELOG/#version-071-tool_main-1070-stacking-020-keras-030-06-11-2019","text":"","title":"Version 0.7.1 / tool_main: 1.0.7.0 / stacking: 0.2.0 / keras: 0.3.0 (06-11-2019)"},{"location":"CHANGELOG/#new-features_3","text":"Adds validation_data into keras galaxy models and supports gridsearch and model_validation . Adds fasta sequence batch generator and makes FastaDNABatchGenerator , FastaRNABatchGenerator and FastaProteinBatchGenerator . Adds keras galaxy batch classifier and generator.flow . Adds keras_batch_models tool. Adds GenomeOneHotEncoder and ProteinOneHotEncoder to pipeline . Adds API documentation at https://goeckslab.github.io/Galaxy-ML/ . Extends BinarizeTarget Classifier/Regressor to support fit_params . Modifies read_columns function to avoid repeated input file reading.","title":"New Features"},{"location":"CHANGELOG/#changes_4","text":"Changes model_validation tool name.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_4","text":"Fixes CV groups file issue in searchcv tool. Fixes cheetah error in model_validation tool.","title":"Bug Fixes"},{"location":"CHANGELOG/#version-065-tool_main-1062-stacking-020-keras-020-05-26-2019","text":"","title":"Version 0.6.5 / tool_main: 1.0.6.2 / stacking: 0.2.0 / keras: 0.2.0 (05-26-2019)"},{"location":"CHANGELOG/#new-features_4","text":"Adds BinarizeTargetTransformer . Adds support of binarize_scorers to BaseSearchCV . Adds sklearn.ensemble.VotingClassifier and VotingRegressor (will be available sklearn v0.21). Enhances security of try_get_attr by adding check_def argument. Adds __all__ attribute together with try_get_attr to manage custom module and names. Adds keras callbacks. Now supports EarlyStopping , RemoteMonitor , TerminateOnNaN , ReduceLROnPlateau and partially support ModelCheckpoint , CSVLogger .","title":"New Features"},{"location":"CHANGELOG/#changes_5","text":"Pumps stacking_ensembles too to version 0.2.0. Changes KerasBatchClassifier to KerasGBatchClassifier .","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_5","text":"Fix voting estimators duplicate naming problem.","title":"Bug Fixes"},{"location":"CHANGELOG/#version-060-tool_main-1060-keras-020-05-13-2019","text":"","title":"Version 0.6.0 / tool_main: 1.0.6.0 / keras: 0.2.0 (05-13-2019)"},{"location":"CHANGELOG/#new-features_5","text":"Adds Nested CV to searchcv tool. Adds BinarizeTargetClassifier .classifier_, BinarizeTargetRegressor .regressor_ and IRAPSClassifier .get_signature() in estimator_attributes tool. Reformat the output of corss_validate . Adds KerasBatchClassifier . Makes KerasGClassifier and KerasGRegressor support multi-dimension array.","title":"New Features"},{"location":"CHANGELOG/#changes_6","text":"Changes min value of n_splits from 2 to 1. Main Tool version changes on the last second number instead of the last one.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_6","text":"Fixes train_test_split which didn't work with default scoring.","title":"Bug Fixes"},{"location":"CHANGELOG/#version-050-tool_main-1005-keras-020-05-13-2019","text":"","title":"Version 0.5.0 / tool_main: 1.0.0.5 / keras: 0.2.0 (05-13-2019)"},{"location":"CHANGELOG/#new-features_6","text":"Extend binarize target scorers to support stacking estimators, i.e., use binarize target estimator as meta estimator. Adds cv_results attributes to estimator_attributes tool. Adds loading prefitted model for prediction in keras_model_builder tool. Adds save_weights and load_weights for keras classifier/regressor models. Merges keras model builder","title":"New Features"},{"location":"CHANGELOG/#changes_7","text":"Refactors the multiple scoring input for searchcv and simplify cv_results output. Refactors import system, get rid of exec import.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_7","text":"Fixes stacking estimators whitelist issue and other import issues. Fixes bases typo error in stacking ensembles tool Fixes multiple scoring error in train_test_split mode","title":"Bug Fixes"},{"location":"CHANGELOG/#version-040-tool_main-1004-04-29-2019","text":"","title":"Version 0.4.0 / tool_main: 1.0.0.4 (04-29-2019)"},{"location":"CHANGELOG/#new-features_7","text":"Adds StackingCVClassifier , StackingClassifier and StackingRegressor to Stacking_ensembles tool, and makes explicit base estimator and meta estimator building options. Adds .gitattributes and .gitignore .","title":"New Features"},{"location":"CHANGELOG/#changes_8","text":"Changes extended_ensemble_ml.xml to stacking_ensembles.xml . Moves src to subfolder Galaxy-ML","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_8","text":"Fix safepickler classobj issue","title":"Bug Fixes"},{"location":"CHANGELOG/#version-030-tool_main-1003-04-23-2019","text":"","title":"Version 0.3.0/ tool_main: 1.0.0.3 (04-23-2019)"},{"location":"CHANGELOG/#new-features_8","text":"Makes RepeatedOrderedKFold . Makes train_test_split tool and adds train_test_split to searchcv tool. Adds jpickle to persist sklearn objects. Makes TDMScaler . Makes search parameter options in search_model_validation tool using from_dataset , the get_params output of estimator_attributes tool. Restructures estimator_attributes tool to be workflow friendly.","title":"New Features"},{"location":"CHANGELOG/#changes_9","text":"Separate OrderedKFold into model_validations module. Refactors SafePickler class and pickle white list loading system for better CPU and Memory efficiency. Separates feature_selector module out from utils .","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_9","text":"Fix safepickler classobj issue","title":"Bug Fixes"},{"location":"CHANGELOG/#version-020-03-24-2019","text":"","title":"Version 0.2.0 (03-24-2019)"},{"location":"CHANGELOG/#new-features_9","text":"SearchCV tool selects param from get_params() dataset. Adds extended_ensemble_ml tool which wraps StackingCVRegressor to ensemble machine learning. Extends estimator_attributes tool to output get_params() . Adds support of multipleprocessing in IRAPSCore .","title":"New Features"},{"location":"CHANGELOG/#changes_10","text":"Removes the limit of n_jobs=1 for IRAPSClassifier Changes named estimators in pipeline_builder tool. Use make_pipeline instead of Pipeline initiation.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_10","text":"","title":"Bug Fixes"},{"location":"CHANGELOG/#version-010-03-15-2019","text":"","title":"Version 0.1.0 (03-15-2019)"},{"location":"APIs/binarize-target/","text":"[source] BinarizeTargetClassifier galaxy_ml.binarize_target._binarize_estimators.BinarizeTargetClassifier(classifier, z_score=-1, value=None, less_is_positive=True, verbose=0) Convert continuous target to binary labels (True and False) and apply a classification estimator. Parameters classifier : object Estimator object such as derived from sklearn ClassifierMixin . z_score : float, default=-1.0 Threshold value based on z_score. Will be ignored when fixed_value is set value : float, default=None Threshold value less_is_positive : boolean, default=True When target is less the threshold value, it will be converted to True, False otherwise. verbose : int, default=0 If greater than 0, print discretizing info. Attributes classifier_ : object Fitted classifier discretize_value : float The threshold value used to discretize True and False targets [source] BinarizeTargetRegressor galaxy_ml.binarize_target._binarize_estimators.BinarizeTargetRegressor(regressor, z_score=-1, value=None, less_is_positive=True, verbose=0) Extend regression estimator to have discretize_value Parameters regressor : object Estimator object such as derived from sklearn RegressionMixin . z_score : float, default=-1.0 Threshold value based on z_score. Will be ignored when value is set value : float, default=None Threshold value less_is_positive : boolean, default=True When target is less the threshold value, it will be converted to True, False otherwise. verbose : int, default=0 If greater than 0, print discretizing info. Attributes regressor_ : object Fitted regressor discretize_value : float The threshold value used to discretize True and False targets [source] BinarizeTargetTransformer galaxy_ml.binarize_target._binarize_estimators.BinarizeTargetTransformer(transformer, z_score=-1, value=None, less_is_positive=True) Extend transformaer to work for binarized target. Parameters transformer : object Estimator object such as derived from sklearn TransformerMixin , including feature_selector and preprocessor z_score : float, default=-1.0 Threshold value based on z_score. Will be ignored when fixed_value is set value : float, default=None Threshold value less_is_positive : boolean, default=True When target is less the threshold value, it will be converted to True, False otherwise. Attributes transformer_ : object Fitted regressor discretize_value : float The threshold value used to discretize True and False targets","title":"binarize estimators/scorers"},{"location":"APIs/binarize-target/#binarizetargetclassifier","text":"galaxy_ml.binarize_target._binarize_estimators.BinarizeTargetClassifier(classifier, z_score=-1, value=None, less_is_positive=True, verbose=0) Convert continuous target to binary labels (True and False) and apply a classification estimator. Parameters classifier : object Estimator object such as derived from sklearn ClassifierMixin . z_score : float, default=-1.0 Threshold value based on z_score. Will be ignored when fixed_value is set value : float, default=None Threshold value less_is_positive : boolean, default=True When target is less the threshold value, it will be converted to True, False otherwise. verbose : int, default=0 If greater than 0, print discretizing info. Attributes classifier_ : object Fitted classifier discretize_value : float The threshold value used to discretize True and False targets [source]","title":"BinarizeTargetClassifier"},{"location":"APIs/binarize-target/#binarizetargetregressor","text":"galaxy_ml.binarize_target._binarize_estimators.BinarizeTargetRegressor(regressor, z_score=-1, value=None, less_is_positive=True, verbose=0) Extend regression estimator to have discretize_value Parameters regressor : object Estimator object such as derived from sklearn RegressionMixin . z_score : float, default=-1.0 Threshold value based on z_score. Will be ignored when value is set value : float, default=None Threshold value less_is_positive : boolean, default=True When target is less the threshold value, it will be converted to True, False otherwise. verbose : int, default=0 If greater than 0, print discretizing info. Attributes regressor_ : object Fitted regressor discretize_value : float The threshold value used to discretize True and False targets [source]","title":"BinarizeTargetRegressor"},{"location":"APIs/binarize-target/#binarizetargettransformer","text":"galaxy_ml.binarize_target._binarize_estimators.BinarizeTargetTransformer(transformer, z_score=-1, value=None, less_is_positive=True) Extend transformaer to work for binarized target. Parameters transformer : object Estimator object such as derived from sklearn TransformerMixin , including feature_selector and preprocessor z_score : float, default=-1.0 Threshold value based on z_score. Will be ignored when fixed_value is set value : float, default=None Threshold value less_is_positive : boolean, default=True When target is less the threshold value, it will be converted to True, False otherwise. Attributes transformer_ : object Fitted regressor discretize_value : float The threshold value used to discretize True and False targets","title":"BinarizeTargetTransformer"},{"location":"APIs/feature-selectors/","text":"[source] DyRFE galaxy_ml.feature_selectors.DyRFE(estimator, n_features_to_select=None, step=1, verbose=0) Mainly used with DyRFECV Parameters estimator : object A supervised learning estimator with a fit method that provides information about feature importance either through a coef_ attribute or through a feature_importances_ attribute. n_features_to_select : int or None (default=None) The number of features to select. If None , half of the features are selected. step : int, float or list, optional (default=1) If greater than or equal to 1, then step corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of features to remove at each iteration. If list, a series of steps of features to remove at each iteration. Iterations stops when steps finish verbose : int, (default=0) Controls verbosity of output. [source] DyRFECV galaxy_ml.feature_selectors.DyRFECV(estimator, step=1, min_features_to_select=1, cv='warn', scoring=None, verbose=0, n_jobs=None) Compared with RFECV, DyRFECV offers flexiable step to eleminate features, in the format of list, while RFECV supports only fixed number of step . Parameters estimator : object A supervised learning estimator with a fit method that provides information about feature importance either through a coef_ attribute or through a feature_importances_ attribute. step : int or float, optional (default=1) If greater than or equal to 1, then step corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of features to remove at each iteration. If list, a series of step to remove at each iteration. iteration stopes when finishing all steps Note that the last iteration may remove fewer than step features in order to reach min_features_to_select . min_features_to_select : int, (default=1) The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and min_features_to_select isn't divisible by step . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - :term: CV splitter , - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used. If the estimator is a classifier or if ``y`` is neither binary nor multiclass, :class:`sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. .. versionchanged:: 0.20 ``cv`` default value of None will change from 3-fold to 5-fold in v0.22. scoring : string, callable or None, optional, (default=None) A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y) . verbose : int, (default=0) Controls verbosity of output. n_jobs : int or None, optional (default=None) Number of cores to run in parallel while fitting across folds. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details.","title":"feature_selectors"},{"location":"APIs/feature-selectors/#dyrfe","text":"galaxy_ml.feature_selectors.DyRFE(estimator, n_features_to_select=None, step=1, verbose=0) Mainly used with DyRFECV Parameters estimator : object A supervised learning estimator with a fit method that provides information about feature importance either through a coef_ attribute or through a feature_importances_ attribute. n_features_to_select : int or None (default=None) The number of features to select. If None , half of the features are selected. step : int, float or list, optional (default=1) If greater than or equal to 1, then step corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of features to remove at each iteration. If list, a series of steps of features to remove at each iteration. Iterations stops when steps finish verbose : int, (default=0) Controls verbosity of output. [source]","title":"DyRFE"},{"location":"APIs/feature-selectors/#dyrfecv","text":"galaxy_ml.feature_selectors.DyRFECV(estimator, step=1, min_features_to_select=1, cv='warn', scoring=None, verbose=0, n_jobs=None) Compared with RFECV, DyRFECV offers flexiable step to eleminate features, in the format of list, while RFECV supports only fixed number of step . Parameters estimator : object A supervised learning estimator with a fit method that provides information about feature importance either through a coef_ attribute or through a feature_importances_ attribute. step : int or float, optional (default=1) If greater than or equal to 1, then step corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage (rounded down) of features to remove at each iteration. If list, a series of step to remove at each iteration. iteration stopes when finishing all steps Note that the last iteration may remove fewer than step features in order to reach min_features_to_select . min_features_to_select : int, (default=1) The minimum number of features to be selected. This number of features will always be scored, even if the difference between the original feature count and min_features_to_select isn't divisible by step . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - :term: CV splitter , - An iterable yielding (train, test) splits as arrays of indices. For integer/None inputs, if ``y`` is binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used. If the estimator is a classifier or if ``y`` is neither binary nor multiclass, :class:`sklearn.model_selection.KFold` is used. Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. .. versionchanged:: 0.20 ``cv`` default value of None will change from 3-fold to 5-fold in v0.22. scoring : string, callable or None, optional, (default=None) A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y) . verbose : int, (default=0) Controls verbosity of output. n_jobs : int or None, optional (default=None) Number of cores to run in parallel while fitting across folds. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details.","title":"DyRFECV"},{"location":"APIs/iraps-classifier/","text":"[source] IRAPSCore galaxy_ml.binarize_target._iraps_classifier.IRAPSCore(n_iter=1000, positive_thres=-1, negative_thres=0, verbose=0, n_jobs=1, pre_dispatch='2*n_jobs', random_state=None, parallel_backend=None) Base class of IRAPSClassifier From sklearn BaseEstimator: get_params() set_params() Parameters n_iter : int sample count positive_thres : float z_score shreshold to discretize positive target values negative_thres : float z_score threshold to discretize negative target values verbose : int 0 or geater, if not 0, print progress n_jobs : int, default=1 The number of CPUs to use to do the computation. pre_dispatch : int, or string. Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' random_state : int or None parallel_backend : str, default is None. Used in joblib.Parallel . Common options are 'loky', 'multiprocessing', and 'threading'. If None, loky is used. [source] IRAPSClassifier galaxy_ml.binarize_target._iraps_classifier.IRAPSClassifier(iraps_core, p_thres=0.0001, fc_thres=0.1, occurrence=0.8, discretize=-1, memory=None, min_signature_features=1) Extend the bases of both sklearn feature_selector and classifier. From sklearn BaseEstimator: get_params() set_params() From sklearn _BaseFilter: get_support() fit_transform(X) transform(X) From sklearn RegressorMixin: score(X, y): R2 New: predict(X) predict_label(X) get_signature() Properties: discretize_value Parameters iraps_core : object p_thres : float, threshold for p_values fc_thres : float, threshold for fold change or mean difference occurrence : float, occurrence rate selected by set of p_thres and fc_thres discretize : float, threshold of z_score to discretize target value memory : None, str or joblib.Memory object min_signature_features : int, the mininum number of features in a signature","title":"iraps_classifier"},{"location":"APIs/iraps-classifier/#irapscore","text":"galaxy_ml.binarize_target._iraps_classifier.IRAPSCore(n_iter=1000, positive_thres=-1, negative_thres=0, verbose=0, n_jobs=1, pre_dispatch='2*n_jobs', random_state=None, parallel_backend=None) Base class of IRAPSClassifier From sklearn BaseEstimator: get_params() set_params() Parameters n_iter : int sample count positive_thres : float z_score shreshold to discretize positive target values negative_thres : float z_score threshold to discretize negative target values verbose : int 0 or geater, if not 0, print progress n_jobs : int, default=1 The number of CPUs to use to do the computation. pre_dispatch : int, or string. Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' random_state : int or None parallel_backend : str, default is None. Used in joblib.Parallel . Common options are 'loky', 'multiprocessing', and 'threading'. If None, loky is used. [source]","title":"IRAPSCore"},{"location":"APIs/iraps-classifier/#irapsclassifier","text":"galaxy_ml.binarize_target._iraps_classifier.IRAPSClassifier(iraps_core, p_thres=0.0001, fc_thres=0.1, occurrence=0.8, discretize=-1, memory=None, min_signature_features=1) Extend the bases of both sklearn feature_selector and classifier. From sklearn BaseEstimator: get_params() set_params() From sklearn _BaseFilter: get_support() fit_transform(X) transform(X) From sklearn RegressorMixin: score(X, y): R2 New: predict(X) predict_label(X) get_signature() Properties: discretize_value Parameters iraps_core : object p_thres : float, threshold for p_values fc_thres : float, threshold for fold change or mean difference occurrence : float, occurrence rate selected by set of p_thres and fc_thres discretize : float, threshold of z_score to discretize target value memory : None, str or joblib.Memory object min_signature_features : int, the mininum number of features in a signature","title":"IRAPSClassifier"},{"location":"APIs/keras-galaxy-models/","text":"[source] SearchParam galaxy_ml.keras_galaxy_models.SearchParam(s_param, value) Sortable Wrapper class for search parameters [source] KerasLayers galaxy_ml.keras_galaxy_models.KerasLayers(name='sequential_1', layers=[]) Parameters name: str layers: list of dict, the configuration of model [source] BaseKerasModel galaxy_ml.keras_galaxy_models.BaseKerasModel(config, model_type='sequential', optimizer='sgd', loss='binary_crossentropy', metrics=[], lr=None, momentum=None, decay=None, nesterov=None, rho=None, amsgrad=None, beta_1=None, beta_2=None, schedule_decay=None, epochs=1, batch_size=None, seed=None, callbacks=None, validation_data=None, steps_per_epoch=None, validation_steps=None, verbose=0) Base class for Galaxy Keras wrapper Parameters config : dictionary from model.get_config() model_type : str 'sequential' or 'functional' optimizer : str, default 'sgd' 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam' loss : str, default 'binary_crossentropy' same as Keras loss metrics : list of strings, default [] lr : None or float optimizer parameter, default change with optimizer momentum : None or float for optimizer sgd only, ignored otherwise nesterov : None or bool for optimizer sgd only, ignored otherwise decay : None or float optimizer parameter, default change with optimizer rho : None or float optimizer parameter, default change with optimizer amsgrad : None or bool for optimizer adam only, ignored otherwise beta_1 : None or float optimizer parameter, default change with optimizer beta_2 : None or float optimizer parameter, default change with optimizer schedule_decay : None or float optimizer parameter, default change with optimizer epochs : int fit_param from Keras batch_size : None or int, default=None fit_param, if None, will default to 32 callbacks : None or list of dict fit_param, each dict contains one type of callback configuration. e.g. {\"callback_selection\": {\"callback_type\": \"EarlyStopping\", \"monitor\": \"val_loss\" \"baseline\": None, \"min_delta\": 0.0, \"patience\": 10, \"mode\": \"auto\", \"restore_best_weights\": False}} validation_data : None or tuple of arrays, (X_test, y_test) fit_param steps_per_epoch : int, default is None fit param. The number of train batches per epoch validation_steps : None or int, default is None fit params, validation steps. if None, it will be number of samples divided by batch_size. seed : None or int, default None backend random seed verbose : 0 or 1 if 1, log device placement [source] KerasGClassifier galaxy_ml.keras_galaxy_models.KerasGClassifier(config, model_type='sequential', optimizer='sgd', loss='binary_crossentropy', metrics=[], lr=None, momentum=None, decay=None, nesterov=None, rho=None, amsgrad=None, beta_1=None, beta_2=None, schedule_decay=None, epochs=1, batch_size=None, seed=None, callbacks=None, validation_data=None, steps_per_epoch=None, validation_steps=None, verbose=0) Scikit-learn classifier API for Keras [source] KerasGRegressor galaxy_ml.keras_galaxy_models.KerasGRegressor(config, model_type='sequential', optimizer='sgd', loss='binary_crossentropy', metrics=[], lr=None, momentum=None, decay=None, nesterov=None, rho=None, amsgrad=None, beta_1=None, beta_2=None, schedule_decay=None, epochs=1, batch_size=None, seed=None, callbacks=None, validation_data=None, steps_per_epoch=None, validation_steps=None, verbose=0) Scikit-learn API wrapper for Keras regressor [source] KerasGBatchClassifier galaxy_ml.keras_galaxy_models.KerasGBatchClassifier(config, data_batch_generator, model_type='sequential', optimizer='sgd', loss='binary_crossentropy', metrics=[], lr=None, momentum=None, decay=None, nesterov=None, rho=None, amsgrad=None, beta_1=None, beta_2=None, schedule_decay=None, epochs=1, batch_size=None, seed=None, n_jobs=1, callbacks=None, validation_data=None, steps_per_epoch=None, validation_steps=None, verbose=0, prediction_steps=None, class_positive_factor=1) keras classifier with batch data generator Parameters config : dictionary from model.get_config() data_batch_generator : instance of batch data generator model_type : str 'sequential' or 'functional' optimizer : str, default 'sgd' 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam' loss : str, default 'binary_crossentropy' same as Keras loss metrics : list of strings, default [] lr : None or float optimizer parameter, default change with optimizer momentum : None or float for optimizer sgd only, ignored otherwise nesterov : None or bool for optimizer sgd only, ignored otherwise decay : None or float optimizer parameter, default change with optimizer rho : None or float optimizer parameter, default change with optimizer amsgrad : None or bool for optimizer adam only, ignored otherwise beta_1 : None or float optimizer parameter, default change with optimizer beta_2 : None or float optimizer parameter, default change with optimizer schedule_decay : None or float optimizer parameter, default change with optimizer epochs : int fit_param from Keras batch_size : None or int, default=None fit_param, if None, will default to 32 callbacks : None or list of dict each dict contains one type of callback configuration. e.g. {\"callback_selection\": {\"callback_type\": \"EarlyStopping\", \"monitor\": \"val_loss\" \"baseline\": None, \"min_delta\": 0.0, \"patience\": 10, \"mode\": \"auto\", \"restore_best_weights\": False}} validation_data : None or tuple of arrays, (X_test, y_test) fit_param steps_per_epoch : int, default is None fit param. The number of train batches per epoch validation_steps : None or int, default is None fit params, validation steps. if None, it will be number of samples divided by batch_size. seed : None or int, default None backend random seed verbose : 0 or 1 if 1, log device placement. n_jobs : int, default=1 prediction_steps : None or int, default is None prediction steps. If None, it will be number of samples divided by batch_size. class_positive_factor : int or float, default=1 For binary classification only. If int, like 5, will convert to class_weight {0: 1, 1: 5}. If float, 0.2, corresponds to class_weight {0: 1/0.2, 1: 1}","title":"keras_galaxy_models"},{"location":"APIs/keras-galaxy-models/#searchparam","text":"galaxy_ml.keras_galaxy_models.SearchParam(s_param, value) Sortable Wrapper class for search parameters [source]","title":"SearchParam"},{"location":"APIs/keras-galaxy-models/#keraslayers","text":"galaxy_ml.keras_galaxy_models.KerasLayers(name='sequential_1', layers=[]) Parameters name: str layers: list of dict, the configuration of model [source]","title":"KerasLayers"},{"location":"APIs/keras-galaxy-models/#basekerasmodel","text":"galaxy_ml.keras_galaxy_models.BaseKerasModel(config, model_type='sequential', optimizer='sgd', loss='binary_crossentropy', metrics=[], lr=None, momentum=None, decay=None, nesterov=None, rho=None, amsgrad=None, beta_1=None, beta_2=None, schedule_decay=None, epochs=1, batch_size=None, seed=None, callbacks=None, validation_data=None, steps_per_epoch=None, validation_steps=None, verbose=0) Base class for Galaxy Keras wrapper Parameters config : dictionary from model.get_config() model_type : str 'sequential' or 'functional' optimizer : str, default 'sgd' 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam' loss : str, default 'binary_crossentropy' same as Keras loss metrics : list of strings, default [] lr : None or float optimizer parameter, default change with optimizer momentum : None or float for optimizer sgd only, ignored otherwise nesterov : None or bool for optimizer sgd only, ignored otherwise decay : None or float optimizer parameter, default change with optimizer rho : None or float optimizer parameter, default change with optimizer amsgrad : None or bool for optimizer adam only, ignored otherwise beta_1 : None or float optimizer parameter, default change with optimizer beta_2 : None or float optimizer parameter, default change with optimizer schedule_decay : None or float optimizer parameter, default change with optimizer epochs : int fit_param from Keras batch_size : None or int, default=None fit_param, if None, will default to 32 callbacks : None or list of dict fit_param, each dict contains one type of callback configuration. e.g. {\"callback_selection\": {\"callback_type\": \"EarlyStopping\", \"monitor\": \"val_loss\" \"baseline\": None, \"min_delta\": 0.0, \"patience\": 10, \"mode\": \"auto\", \"restore_best_weights\": False}} validation_data : None or tuple of arrays, (X_test, y_test) fit_param steps_per_epoch : int, default is None fit param. The number of train batches per epoch validation_steps : None or int, default is None fit params, validation steps. if None, it will be number of samples divided by batch_size. seed : None or int, default None backend random seed verbose : 0 or 1 if 1, log device placement [source]","title":"BaseKerasModel"},{"location":"APIs/keras-galaxy-models/#kerasgclassifier","text":"galaxy_ml.keras_galaxy_models.KerasGClassifier(config, model_type='sequential', optimizer='sgd', loss='binary_crossentropy', metrics=[], lr=None, momentum=None, decay=None, nesterov=None, rho=None, amsgrad=None, beta_1=None, beta_2=None, schedule_decay=None, epochs=1, batch_size=None, seed=None, callbacks=None, validation_data=None, steps_per_epoch=None, validation_steps=None, verbose=0) Scikit-learn classifier API for Keras [source]","title":"KerasGClassifier"},{"location":"APIs/keras-galaxy-models/#kerasgregressor","text":"galaxy_ml.keras_galaxy_models.KerasGRegressor(config, model_type='sequential', optimizer='sgd', loss='binary_crossentropy', metrics=[], lr=None, momentum=None, decay=None, nesterov=None, rho=None, amsgrad=None, beta_1=None, beta_2=None, schedule_decay=None, epochs=1, batch_size=None, seed=None, callbacks=None, validation_data=None, steps_per_epoch=None, validation_steps=None, verbose=0) Scikit-learn API wrapper for Keras regressor [source]","title":"KerasGRegressor"},{"location":"APIs/keras-galaxy-models/#kerasgbatchclassifier","text":"galaxy_ml.keras_galaxy_models.KerasGBatchClassifier(config, data_batch_generator, model_type='sequential', optimizer='sgd', loss='binary_crossentropy', metrics=[], lr=None, momentum=None, decay=None, nesterov=None, rho=None, amsgrad=None, beta_1=None, beta_2=None, schedule_decay=None, epochs=1, batch_size=None, seed=None, n_jobs=1, callbacks=None, validation_data=None, steps_per_epoch=None, validation_steps=None, verbose=0, prediction_steps=None, class_positive_factor=1) keras classifier with batch data generator Parameters config : dictionary from model.get_config() data_batch_generator : instance of batch data generator model_type : str 'sequential' or 'functional' optimizer : str, default 'sgd' 'sgd', 'rmsprop', 'adagrad', 'adadelta', 'adam', 'adamax', 'nadam' loss : str, default 'binary_crossentropy' same as Keras loss metrics : list of strings, default [] lr : None or float optimizer parameter, default change with optimizer momentum : None or float for optimizer sgd only, ignored otherwise nesterov : None or bool for optimizer sgd only, ignored otherwise decay : None or float optimizer parameter, default change with optimizer rho : None or float optimizer parameter, default change with optimizer amsgrad : None or bool for optimizer adam only, ignored otherwise beta_1 : None or float optimizer parameter, default change with optimizer beta_2 : None or float optimizer parameter, default change with optimizer schedule_decay : None or float optimizer parameter, default change with optimizer epochs : int fit_param from Keras batch_size : None or int, default=None fit_param, if None, will default to 32 callbacks : None or list of dict each dict contains one type of callback configuration. e.g. {\"callback_selection\": {\"callback_type\": \"EarlyStopping\", \"monitor\": \"val_loss\" \"baseline\": None, \"min_delta\": 0.0, \"patience\": 10, \"mode\": \"auto\", \"restore_best_weights\": False}} validation_data : None or tuple of arrays, (X_test, y_test) fit_param steps_per_epoch : int, default is None fit param. The number of train batches per epoch validation_steps : None or int, default is None fit params, validation steps. if None, it will be number of samples divided by batch_size. seed : None or int, default None backend random seed verbose : 0 or 1 if 1, log device placement. n_jobs : int, default=1 prediction_steps : None or int, default is None prediction steps. If None, it will be number of samples divided by batch_size. class_positive_factor : int or float, default=1 For binary classification only. If int, like 5, will convert to class_weight {0: 1, 1: 5}. If float, 0.2, corresponds to class_weight {0: 1/0.2, 1: 1}","title":"KerasGBatchClassifier"},{"location":"APIs/model-persistent/","text":"[source] JPicklerError galaxy_ml.model_persist.JPicklerError() [source] ModelToDict galaxy_ml.model_persist.ModelToDict() Follow the track of python pickle Turn a scikit-learn model to a JSON-compatiable dictionary [source] DictToModel galaxy_ml.model_persist.DictToModel() Rebuild a scikit-learn model from dict data generated by ModelToDict.save","title":"model_persistent"},{"location":"APIs/model-persistent/#jpicklererror","text":"galaxy_ml.model_persist.JPicklerError() [source]","title":"JPicklerError"},{"location":"APIs/model-persistent/#modeltodict","text":"galaxy_ml.model_persist.ModelToDict() Follow the track of python pickle Turn a scikit-learn model to a JSON-compatiable dictionary [source]","title":"ModelToDict"},{"location":"APIs/model-persistent/#dicttomodel","text":"galaxy_ml.model_persist.DictToModel() Rebuild a scikit-learn model from dict data generated by ModelToDict.save","title":"DictToModel"},{"location":"APIs/model-validations/","text":"[source] OrderedKFold galaxy_ml.model_validations.OrderedKFold(n_splits=3, shuffle=False, random_state=None) Split into K fold based on ordered target value Parameters n_splits : int, default=3 Number of folds. Must be at least 2. shuffle : bool random_state : None or int [source] RepeatedOrderedKFold galaxy_ml.model_validations.RepeatedOrderedKFold(n_splits=5, n_repeats=5, random_state=None) Repeated OrderedKFold runs mutiple times with different randomization. Parameters n_splits : int, default=5 Number of folds. Must be at least 2. n_repeats : int, default=5 Number of times cross-validator to be repeated. random_state: int, RandomState instance or None. Optional","title":"model_validations"},{"location":"APIs/model-validations/#orderedkfold","text":"galaxy_ml.model_validations.OrderedKFold(n_splits=3, shuffle=False, random_state=None) Split into K fold based on ordered target value Parameters n_splits : int, default=3 Number of folds. Must be at least 2. shuffle : bool random_state : None or int [source]","title":"OrderedKFold"},{"location":"APIs/model-validations/#repeatedorderedkfold","text":"galaxy_ml.model_validations.RepeatedOrderedKFold(n_splits=5, n_repeats=5, random_state=None) Repeated OrderedKFold runs mutiple times with different randomization. Parameters n_splits : int, default=5 Number of folds. Must be at least 2. n_repeats : int, default=5 Number of times cross-validator to be repeated. random_state: int, RandomState instance or None. Optional","title":"RepeatedOrderedKFold"},{"location":"APIs/preprocessors/","text":"[source] Z_RandomOverSampler galaxy_ml.preprocessors._z_random_over_sampler.Z_RandomOverSampler(sampling_strategy='auto', return_indices=False, random_state=None, ratio=None, negative_thres=0, positive_thres=-1) [source] TDMScaler galaxy_ml.preprocessors._tdm_scaler.TDMScaler(q_lower=25.0, q_upper=75.0) Scale features using Training Distribution Matching (TDM) algorithm References .. [1] Thompson JA, Tan J and Greene CS (2016) Cross-platform normalization of microarray and RNA-seq data for machine learning applications. PeerJ 4, e1621. [source] GenomeOneHotEncoder galaxy_ml.preprocessors._genome_one_hot_encoder.GenomeOneHotEncoder(fasta_path=None, padding=True, seq_length=None) Convert Genomic sequences to one-hot encoded 2d array Paramaters fasta_path : str, default None File path to the fasta file. There could two other ways to set up fasta_path . 1) through fit_params; 2) set_params(). If fasta_path is None, we suppose the sequences are contained in first column of X. padding : bool, default is False All sequences are expected to be in the same length, but sometimes not. If True, all sequences use the same length of first entry by either padding or truncating. If False, raise ValueError if different seuqnce lengths are found. seq_length : None or int Sequence length. If None, determined by the the first entry. [source] ProteinOneHotEncoder galaxy_ml.preprocessors._genome_one_hot_encoder.ProteinOneHotEncoder(fasta_path=None, padding=True, seq_length=None) Convert protein sequences to one-hot encoded 2d array Paramaters fasta_path : str, default None File path to the fasta file. There could two other ways to set up fasta_path . 1) through fit_params; 2) set_params(). If fasta_path is None, we suppose the sequences are contained in first column of X. padding : bool, default is False All sequences are expected to be in the same length, but sometimes not. If True, all sequences use the same length of first entry by either padding or truncating. If False, raise ValueError if different seuqnce lengths are found. seq_length : None or int Sequence length. If None, determined by the the first entry. [source] FastaIterator galaxy_ml.preprocessors._fasta_iterator.FastaIterator(n, batch_size=32, shuffle=True, seed=0) Base class for fasta sequence iterators. Parameters n : int Total number of samples batch_size : int Size of batch shuffle : bool Whether to shuffle data between epoch seed : int Random seed number for data shuffling [source] FastaToArrayIterator galaxy_ml.preprocessors._fasta_iterator.FastaToArrayIterator(X, generator, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None) Iterator yielding Numpy array from fasta sequences Parameters X : array Contains sequence indexes in the fasta file generator : fitted object instance of BatchGenerator, e.g., FastaDNABatchGenerator or FastaProteinBatchGenerator y : array Target labels or values batch_size : int, default=32 shuffle : bool, default=True Whether to shuffle the data between epochs sample_weight : None or array Sample weight seed : int Random seed for data shuffling [source] FastaDNABatchGenerator galaxy_ml.preprocessors._fasta_dna_batch_generator.FastaDNABatchGenerator(fasta_path, seq_length=1000, shuffle=True, seed=None) Fasta squence batch data generator, online transformation of sequences to array. Parameters fasta_path : str File path to fasta file. seq_length : int, default=1000 Sequence length, number of bases. shuffle : bool, default=True Whether to shuffle the data between epochs seed : int Random seed for data shuffling [source] FastaRNABatchGenerator galaxy_ml.preprocessors._fasta_rna_batch_generator.FastaRNABatchGenerator(fasta_path, seq_length=1000, shuffle=True, seed=None) Fasta squence batch data generator, online transformation of sequences to array. Parameters fasta_path : str File path to fasta file. seq_length : int, default=1000 Sequence length, number of bases. shuffle : bool, default=True Whether to shuffle the data between epochs seed : int Random seed for data shuffling [source] FastaProteinBatchGenerator galaxy_ml.preprocessors._fasta_protein_batch_generator.FastaProteinBatchGenerator(fasta_path, seq_length=1000, shuffle=True, seed=None) Fasta squence batch data generator, online transformation of sequences to array. Parameters fasta_path : str File path to fasta file. seq_length : int, default=1000 Sequence length, number of bases. shuffle : bool, default=True Whether to shuffle the data between epochs seed : int Random seed for data shuffling [source] IntervalsToArrayIterator galaxy_ml.preprocessors._genomic_interval_batch_generator.IntervalsToArrayIterator(X, generator, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, sample_probabilities=None) Iterator yielding Numpy array from intervals and reference sequences. Parameters X : array Contains sequence indexes in the fasta file generator : fitted object instance of GenomicIntervalBatchGenerator. y : None The existing of y is due to inheritence, should be always None. batch_size : int, default=32 shuffle : bool, default=True Whether to shuffle the data between epochs sample_weight : None or array Sample weight seed : int Random seed for data shuffling sample_probabilities : 1-D array or None, default is None. The probabilities to draw samples. Different from the sample weight, this parameter only changes the the frequency of sampling, won't the loss during training. [source] GenomicIntervalBatchGenerator galaxy_ml.preprocessors._genomic_interval_batch_generator.GenomicIntervalBatchGenerator(ref_genome_path=None, intervals_path=None, target_path=None, features='infer', blacklist_regions='hg38', shuffle=True, seed=None, seq_length=1000, center_bin_to_predict=200, feature_thresholds=0.5, random_state=None) Generate sequence array and target values from a reference genome, intervals and genomic feature dataset. Try to mimic the the selene_sdk.samplers.interval_sampler.IntervalsSampler . Parameters ref_genome_path : str File path to the reference genomce, usually in fasta format. intervals_path : str File path to the intervals dataset. target_path : str File path to the dataset containing genomic features or target information, usually in bed ir bed.gz format. features : list of str or 'infer' A list of features to predict. If 'infer', retrieve all the unique features from the target file. blacklist_regions : str E.g., 'hg38'. For more info, refer to selene_sdk.sequences.Genome . shuffle : bool, default=True Whether to shuffle the data between epochs. seed : int or None, default=None Random seed for shuffling between epocks. seq_length : int, default=1000 Retrived sequence length. center_bin_to_predict : int, default=200 Query the tabix-indexed file for a region of length center_bin_to_predict . feature_thresholds : float, default=0.5 Threshold values to determine target value. random_state : int or None, default=None Random seed for sampling sequences with changing position. [source] GenomicVariantBatchGenerator galaxy_ml.preprocessors._genomic_variant_batch_generator.GenomicVariantBatchGenerator(ref_genome_path=None, vcf_path=None, blacklist_regions='hg38', seq_length=1000, output_reference=False) keras.utils.Sequence capable sequence array generator from a reference genome and VCF (variant call format) file. Parameters ref_genome_path : str File path to the reference genomce, usually in fasta format. vcf_path : str File path to the VCF dataset. blacklist_regions : str E.g., 'hg38'. For more info, refer to selene_sdk.sequences.Genome . seq_length : int, default=1000 Retrived sequence length. output_reference : bool, default is False. If True, output reference sequence instead. [source] ImageDataFrameBatchGenerator galaxy_ml.preprocessors._image_batch_generator.ImageDataFrameBatchGenerator(dataframe, featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format='channels_last', interpolation_order=1, dtype='float32', directory=None, x_col='filename', y_col='class', weight_col=None, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', interpolation='nearest', fit_sample_size=None) Extend keras_preprocessing.image.ImageDataGenerator to work with DataFrame exclusively, generating batches of tensor data from images with online augumentation. Parameters From `keras_preprocessing.image.ImageDataGenerator`. featurewise_center : Boolean. Set input mean to 0 over the dataset, feature-wise. samplewise_center : Boolean. Set each sample mean to 0. featurewise_std_normalization : Boolean. Divide inputs by std of the dataset, feature-wise. samplewise_std_normalization : Boolean. Divide each input by its std. zca_whitening : Boolean. Apply ZCA whitening. zca_epsilon : epsilon for ZCA whitening. Default is 1e-6. rotation_range : Int. Degree range for random rotations. width_shift_range : Float, 1-D array-like or int. height_shift_range : Float, 1-D array-like or int. brightness_range : Tuple or list of two floats. shear_range : Float. Shear Intensity. zoom_range : Float or [lower, upper]. channel_shift_range : Float. Range for random channel shifts. fill_mode : One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default is 'nearest'. Points outside the boundaries of the input are filled according to the given mode: - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k) - 'nearest': aaaaaaaa|abcd|dddddddd - 'reflect': abcddcba|abcd|dcbaabcd - 'wrap': abcdabcd|abcd|abcdabcd cval : Float or Int. horizontal_flip : Boolean. Randomly flip inputs horizontally. vertical_flip : Boolean. rescale : rescaling factor. Defaults to None. preprocessing_function : function that will be applied on each input. The function will run after the image is resized and augmented. The function should take one argument: one image (Numpy tensor with rank 3), and should output a Numpy tensor with the same shape. data_format : Image data format, either \"channels_first\" or \"channels_last\". \"channels_last\" mode means that the images should have shape (samples, height, width, channels) , \"channels_first\" mode means that the images should have shape (samples, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". interpolation_order : Int. dtype : Dtype to use for the generated arrays. Default is 'float32'. dataframe : Pandas dataframe containing the filepaths relative to directory . From keras_preprocessing.image.ImageDataGenerator. flow_from_dataframe . directory : string, path to the directory to read images from. If None , data in x_col column should be absolute paths. x_col : string, column in dataframe that contains the filenames (or absolute paths if directory is None ). y_col : string or list, column/s in dataframe that has the target data. weight_col : string, column in dataframe that contains the sample weights. Default: None . target_size : tuple of integers (height, width) , default: (256, 256) . The dimensions to which all images found will be resized. color_mode : one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1 or 3 color channels. classes : optional list of classes (e.g. ['dogs', 'cats'] ). Default: None. If not provided, the list of classes will be automatically inferred from the y_col , which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute class_indices . class_mode : one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None. Default: \"categorical\". Mode for yielding the targets: - \"binary\" : 1D numpy array of binary labels, - \"categorical\" : 2D numpy array of one-hot encoded labels. Supports multi-label output. - \"input\" : images identical to input images (mainly used to work with autoencoders), - \"multi_output\" : list with the values of the different columns, - \"raw\" : numpy array of values in y_col column(s), - \"sparse\" : 1D numpy array of integer labels, - None , no targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict_generator() ). shuffle : whether to shuffle the data (default: True) seed : optional random seed for shuffling and transformations. save_to_dir : Optional directory where to save the pictures being yielded, in a viewable format. This is useful for visualizing the random transformations being applied, for debugging purposes. save_prefix : String prefix to use for saving sample images (if save_to_dir is set). save_format : Format to use for saving sample images (if save_to_dir is set). interpolation : Interpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are \"nearest\" , \"bilinear\" , and \"bicubic\" . If PIL version 1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0 or newer is installed, \"box\" and \"hamming\" are also supported. By default, \"nearest\" is used. fit_sample_size : Int. Default is None / 1000. Number of training images used in datagen.fit . Relevant only when featurewise_center or featurewise_std_normalization or zca_whitening are set are set to True.","title":"preprocessors"},{"location":"APIs/preprocessors/#z_randomoversampler","text":"galaxy_ml.preprocessors._z_random_over_sampler.Z_RandomOverSampler(sampling_strategy='auto', return_indices=False, random_state=None, ratio=None, negative_thres=0, positive_thres=-1) [source]","title":"Z_RandomOverSampler"},{"location":"APIs/preprocessors/#tdmscaler","text":"galaxy_ml.preprocessors._tdm_scaler.TDMScaler(q_lower=25.0, q_upper=75.0) Scale features using Training Distribution Matching (TDM) algorithm References .. [1] Thompson JA, Tan J and Greene CS (2016) Cross-platform normalization of microarray and RNA-seq data for machine learning applications. PeerJ 4, e1621. [source]","title":"TDMScaler"},{"location":"APIs/preprocessors/#genomeonehotencoder","text":"galaxy_ml.preprocessors._genome_one_hot_encoder.GenomeOneHotEncoder(fasta_path=None, padding=True, seq_length=None) Convert Genomic sequences to one-hot encoded 2d array Paramaters fasta_path : str, default None File path to the fasta file. There could two other ways to set up fasta_path . 1) through fit_params; 2) set_params(). If fasta_path is None, we suppose the sequences are contained in first column of X. padding : bool, default is False All sequences are expected to be in the same length, but sometimes not. If True, all sequences use the same length of first entry by either padding or truncating. If False, raise ValueError if different seuqnce lengths are found. seq_length : None or int Sequence length. If None, determined by the the first entry. [source]","title":"GenomeOneHotEncoder"},{"location":"APIs/preprocessors/#proteinonehotencoder","text":"galaxy_ml.preprocessors._genome_one_hot_encoder.ProteinOneHotEncoder(fasta_path=None, padding=True, seq_length=None) Convert protein sequences to one-hot encoded 2d array Paramaters fasta_path : str, default None File path to the fasta file. There could two other ways to set up fasta_path . 1) through fit_params; 2) set_params(). If fasta_path is None, we suppose the sequences are contained in first column of X. padding : bool, default is False All sequences are expected to be in the same length, but sometimes not. If True, all sequences use the same length of first entry by either padding or truncating. If False, raise ValueError if different seuqnce lengths are found. seq_length : None or int Sequence length. If None, determined by the the first entry. [source]","title":"ProteinOneHotEncoder"},{"location":"APIs/preprocessors/#fastaiterator","text":"galaxy_ml.preprocessors._fasta_iterator.FastaIterator(n, batch_size=32, shuffle=True, seed=0) Base class for fasta sequence iterators. Parameters n : int Total number of samples batch_size : int Size of batch shuffle : bool Whether to shuffle data between epoch seed : int Random seed number for data shuffling [source]","title":"FastaIterator"},{"location":"APIs/preprocessors/#fastatoarrayiterator","text":"galaxy_ml.preprocessors._fasta_iterator.FastaToArrayIterator(X, generator, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None) Iterator yielding Numpy array from fasta sequences Parameters X : array Contains sequence indexes in the fasta file generator : fitted object instance of BatchGenerator, e.g., FastaDNABatchGenerator or FastaProteinBatchGenerator y : array Target labels or values batch_size : int, default=32 shuffle : bool, default=True Whether to shuffle the data between epochs sample_weight : None or array Sample weight seed : int Random seed for data shuffling [source]","title":"FastaToArrayIterator"},{"location":"APIs/preprocessors/#fastadnabatchgenerator","text":"galaxy_ml.preprocessors._fasta_dna_batch_generator.FastaDNABatchGenerator(fasta_path, seq_length=1000, shuffle=True, seed=None) Fasta squence batch data generator, online transformation of sequences to array. Parameters fasta_path : str File path to fasta file. seq_length : int, default=1000 Sequence length, number of bases. shuffle : bool, default=True Whether to shuffle the data between epochs seed : int Random seed for data shuffling [source]","title":"FastaDNABatchGenerator"},{"location":"APIs/preprocessors/#fastarnabatchgenerator","text":"galaxy_ml.preprocessors._fasta_rna_batch_generator.FastaRNABatchGenerator(fasta_path, seq_length=1000, shuffle=True, seed=None) Fasta squence batch data generator, online transformation of sequences to array. Parameters fasta_path : str File path to fasta file. seq_length : int, default=1000 Sequence length, number of bases. shuffle : bool, default=True Whether to shuffle the data between epochs seed : int Random seed for data shuffling [source]","title":"FastaRNABatchGenerator"},{"location":"APIs/preprocessors/#fastaproteinbatchgenerator","text":"galaxy_ml.preprocessors._fasta_protein_batch_generator.FastaProteinBatchGenerator(fasta_path, seq_length=1000, shuffle=True, seed=None) Fasta squence batch data generator, online transformation of sequences to array. Parameters fasta_path : str File path to fasta file. seq_length : int, default=1000 Sequence length, number of bases. shuffle : bool, default=True Whether to shuffle the data between epochs seed : int Random seed for data shuffling [source]","title":"FastaProteinBatchGenerator"},{"location":"APIs/preprocessors/#intervalstoarrayiterator","text":"galaxy_ml.preprocessors._genomic_interval_batch_generator.IntervalsToArrayIterator(X, generator, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, sample_probabilities=None) Iterator yielding Numpy array from intervals and reference sequences. Parameters X : array Contains sequence indexes in the fasta file generator : fitted object instance of GenomicIntervalBatchGenerator. y : None The existing of y is due to inheritence, should be always None. batch_size : int, default=32 shuffle : bool, default=True Whether to shuffle the data between epochs sample_weight : None or array Sample weight seed : int Random seed for data shuffling sample_probabilities : 1-D array or None, default is None. The probabilities to draw samples. Different from the sample weight, this parameter only changes the the frequency of sampling, won't the loss during training. [source]","title":"IntervalsToArrayIterator"},{"location":"APIs/preprocessors/#genomicintervalbatchgenerator","text":"galaxy_ml.preprocessors._genomic_interval_batch_generator.GenomicIntervalBatchGenerator(ref_genome_path=None, intervals_path=None, target_path=None, features='infer', blacklist_regions='hg38', shuffle=True, seed=None, seq_length=1000, center_bin_to_predict=200, feature_thresholds=0.5, random_state=None) Generate sequence array and target values from a reference genome, intervals and genomic feature dataset. Try to mimic the the selene_sdk.samplers.interval_sampler.IntervalsSampler . Parameters ref_genome_path : str File path to the reference genomce, usually in fasta format. intervals_path : str File path to the intervals dataset. target_path : str File path to the dataset containing genomic features or target information, usually in bed ir bed.gz format. features : list of str or 'infer' A list of features to predict. If 'infer', retrieve all the unique features from the target file. blacklist_regions : str E.g., 'hg38'. For more info, refer to selene_sdk.sequences.Genome . shuffle : bool, default=True Whether to shuffle the data between epochs. seed : int or None, default=None Random seed for shuffling between epocks. seq_length : int, default=1000 Retrived sequence length. center_bin_to_predict : int, default=200 Query the tabix-indexed file for a region of length center_bin_to_predict . feature_thresholds : float, default=0.5 Threshold values to determine target value. random_state : int or None, default=None Random seed for sampling sequences with changing position. [source]","title":"GenomicIntervalBatchGenerator"},{"location":"APIs/preprocessors/#genomicvariantbatchgenerator","text":"galaxy_ml.preprocessors._genomic_variant_batch_generator.GenomicVariantBatchGenerator(ref_genome_path=None, vcf_path=None, blacklist_regions='hg38', seq_length=1000, output_reference=False) keras.utils.Sequence capable sequence array generator from a reference genome and VCF (variant call format) file. Parameters ref_genome_path : str File path to the reference genomce, usually in fasta format. vcf_path : str File path to the VCF dataset. blacklist_regions : str E.g., 'hg38'. For more info, refer to selene_sdk.sequences.Genome . seq_length : int, default=1000 Retrived sequence length. output_reference : bool, default is False. If True, output reference sequence instead. [source]","title":"GenomicVariantBatchGenerator"},{"location":"APIs/preprocessors/#imagedataframebatchgenerator","text":"galaxy_ml.preprocessors._image_batch_generator.ImageDataFrameBatchGenerator(dataframe, featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format='channels_last', interpolation_order=1, dtype='float32', directory=None, x_col='filename', y_col='class', weight_col=None, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', interpolation='nearest', fit_sample_size=None) Extend keras_preprocessing.image.ImageDataGenerator to work with DataFrame exclusively, generating batches of tensor data from images with online augumentation. Parameters From `keras_preprocessing.image.ImageDataGenerator`. featurewise_center : Boolean. Set input mean to 0 over the dataset, feature-wise. samplewise_center : Boolean. Set each sample mean to 0. featurewise_std_normalization : Boolean. Divide inputs by std of the dataset, feature-wise. samplewise_std_normalization : Boolean. Divide each input by its std. zca_whitening : Boolean. Apply ZCA whitening. zca_epsilon : epsilon for ZCA whitening. Default is 1e-6. rotation_range : Int. Degree range for random rotations. width_shift_range : Float, 1-D array-like or int. height_shift_range : Float, 1-D array-like or int. brightness_range : Tuple or list of two floats. shear_range : Float. Shear Intensity. zoom_range : Float or [lower, upper]. channel_shift_range : Float. Range for random channel shifts. fill_mode : One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}. Default is 'nearest'. Points outside the boundaries of the input are filled according to the given mode: - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k) - 'nearest': aaaaaaaa|abcd|dddddddd - 'reflect': abcddcba|abcd|dcbaabcd - 'wrap': abcdabcd|abcd|abcdabcd cval : Float or Int. horizontal_flip : Boolean. Randomly flip inputs horizontally. vertical_flip : Boolean. rescale : rescaling factor. Defaults to None. preprocessing_function : function that will be applied on each input. The function will run after the image is resized and augmented. The function should take one argument: one image (Numpy tensor with rank 3), and should output a Numpy tensor with the same shape. data_format : Image data format, either \"channels_first\" or \"channels_last\". \"channels_last\" mode means that the images should have shape (samples, height, width, channels) , \"channels_first\" mode means that the images should have shape (samples, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". interpolation_order : Int. dtype : Dtype to use for the generated arrays. Default is 'float32'. dataframe : Pandas dataframe containing the filepaths relative to directory . From keras_preprocessing.image.ImageDataGenerator. flow_from_dataframe . directory : string, path to the directory to read images from. If None , data in x_col column should be absolute paths. x_col : string, column in dataframe that contains the filenames (or absolute paths if directory is None ). y_col : string or list, column/s in dataframe that has the target data. weight_col : string, column in dataframe that contains the sample weights. Default: None . target_size : tuple of integers (height, width) , default: (256, 256) . The dimensions to which all images found will be resized. color_mode : one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". Whether the images will be converted to have 1 or 3 color channels. classes : optional list of classes (e.g. ['dogs', 'cats'] ). Default: None. If not provided, the list of classes will be automatically inferred from the y_col , which will map to the label indices, will be alphanumeric). The dictionary containing the mapping from class names to class indices can be obtained via the attribute class_indices . class_mode : one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None. Default: \"categorical\". Mode for yielding the targets: - \"binary\" : 1D numpy array of binary labels, - \"categorical\" : 2D numpy array of one-hot encoded labels. Supports multi-label output. - \"input\" : images identical to input images (mainly used to work with autoencoders), - \"multi_output\" : list with the values of the different columns, - \"raw\" : numpy array of values in y_col column(s), - \"sparse\" : 1D numpy array of integer labels, - None , no targets are returned (the generator will only yield batches of image data, which is useful to use in model.predict_generator() ). shuffle : whether to shuffle the data (default: True) seed : optional random seed for shuffling and transformations. save_to_dir : Optional directory where to save the pictures being yielded, in a viewable format. This is useful for visualizing the random transformations being applied, for debugging purposes. save_prefix : String prefix to use for saving sample images (if save_to_dir is set). save_format : Format to use for saving sample images (if save_to_dir is set). interpolation : Interpolation method used to resample the image if the target size is different from that of the loaded image. Supported methods are \"nearest\" , \"bilinear\" , and \"bicubic\" . If PIL version 1.1.3 or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0 or newer is installed, \"box\" and \"hamming\" are also supported. By default, \"nearest\" is used. fit_sample_size : Int. Default is None / 1000. Number of training images used in datagen.fit . Relevant only when featurewise_center or featurewise_std_normalization or zca_whitening are set are set to True.","title":"ImageDataFrameBatchGenerator"},{"location":"APIs/utils/","text":"[source] _SafePickler galaxy_ml.utils._SafePickler(file) Used to safely deserialize scikit-learn model objects Usage: eg.: _SafePickler.load(pickled_file_object) [source] SafeEval galaxy_ml.utils.SafeEval(load_scipy=False, load_numpy=False, load_estimators=False) Customized symbol table for safely literal eval Parameters load_scipy : bool, default=False Whether to load globals from scipy load_numpy : bool, default=False Whether to load globals from numpy load_estimators : bool, default=False Whether to load globals for sklearn estimators","title":"utils"},{"location":"APIs/utils/#_safepickler","text":"galaxy_ml.utils._SafePickler(file) Used to safely deserialize scikit-learn model objects Usage: eg.: _SafePickler.load(pickled_file_object) [source]","title":"_SafePickler"},{"location":"APIs/utils/#safeeval","text":"galaxy_ml.utils.SafeEval(load_scipy=False, load_numpy=False, load_estimators=False) Customized symbol table for safely literal eval Parameters load_scipy : bool, default=False Whether to load globals from scipy load_numpy : bool, default=False Whether to load globals from numpy load_estimators : bool, default=False Whether to load globals for sklearn estimators","title":"SafeEval"}]}